{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e03a43-7920-4679-a955-ed3118ef1201",
   "metadata": {},
   "source": [
    "# Pull request scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a05420a-77b4-4d26-acdd-b6ca46d0f239",
   "metadata": {},
   "source": [
    "## GitHub credentials\n",
    "A private access token is necessary to make use of less restrictive API limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18030255-77ee-4ed0-b11b-b2824e704349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Request GET /user failed with 403: Forbidden\n",
      "Setting next backoff to 82.925177s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: AbelvdTil\n"
     ]
    }
   ],
   "source": [
    "from github import RateLimitExceededException, Github\n",
    "\n",
    "# Providing access token\n",
    "access_token = \"\"\n",
    "g = Github(login_or_token=access_token)\n",
    "\n",
    "# Confirm your login is successful\n",
    "user = g.get_user()\n",
    "print(f\"Authenticated as: {user.login}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09550648-f771-420c-bfbc-d3cd7bceec95",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f5be4139-d47b-42f1-b4a6-eee6d6de92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "STEP4_TFCOMMITS = os.path.join(\"data\", \"previous-study\", \"step4-tf-commits.json\") \n",
    "COMMIT_LABELS = os.path.join(\"data\", \"process-labeled-commits\", \"full-commit-labels.json\") \n",
    "NEW_COMMITS = os.path.join(\"data\", \"update-previous-study\", \"new-commits-dataset.json\") \n",
    "OLD_COMMITS = os.path.join(\"data\", \"previous-study\", \"dataset.json\") \n",
    "\n",
    "STEP5_TF_REPOS_WITH_PR = os.path.join(\"data\", \"pullrequest-scraping\", \"step5-tf-repos-with-pr.json\")\n",
    "STEP6_TF_REPOS_COMMITS = os.path.join(\"data\", \"pullrequest-scraping\", \"step6-tf-repos-commits.json\")\n",
    "STEP6A_TF_REPOS_RELEVANT_COMMITS = os.path.join(\"data\", \"pullrequest-scraping\", \"step6a-tf-repos-relevant-commits.json\")\n",
    "STEP7_TF_REPOS_WITH_TF_PR = os.path.join(\"data\", \"pullrequest-scraping\", \"step7-tf-repos-with-tf-pr.json\")\n",
    "STEP8_TF_KEYWORD_PR = os.path.join(\"data\", \"pullrequest-scraping\", \"step8-tf-keyword-pr.json\")\n",
    "STEP9_TF_PR_DATASET = os.path.join(\"data\", \"pullrequest-scraping\", \"step9-tf-pr-dataset.json\")\n",
    "STEP11_RELEVANT_DATASET = os.path.join(\"data\", \"pullrequest-scraping\", \"step11-relevant-tf-pr-dataset.json\")\n",
    "COMMIT_PR_INTERSECTION = os.path.join(\"data\", \"pullrequest-scraping\", \"commit-pr-intersection.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dc4f3-4c41-422a-af1a-817915c2fb00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## File seperator and combinator\n",
    "\n",
    "Files over 100MB are not stored on GitHub, therefore we need to seperate large files into smaller ones.\n",
    "Any step5 and step7 files can be split into multiple smaller ones and be combined together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8300a-3f1f-4b55-9c2f-319d2213d816",
   "metadata": {},
   "source": [
    "The repositories are split on pull request level. The first pull request is stored in part 1, the 2nd in part 2 etc. This will make sure that it is fairly equally distributed.\n",
    "\n",
    "Each pull request is accomodated with the url of the repository. Therefore it is possible to reconstruct the original file again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d27ae3-61d8-444e-99e5-bfffe7e66185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "FILE_TO_SEPERATE = STEP5_TF_REPOS_WITH_PR\n",
    "\n",
    "nr_parts = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8c9fe-a182-4600-bd3c-6e3fee8607a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### File seperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac6757a0-cacc-4792-87d7-3ade5ff857f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_data = []\n",
    "\n",
    "for i in range(nr_parts):\n",
    "    parts_data.append([])\n",
    "\n",
    "file = open(FILE_TO_SEPERATE)\n",
    "seperator_data = json.load(file)\n",
    "\n",
    "count = 0\n",
    "for repository in seperator_data:\n",
    "    for pull_request in repository[\"pull_requests\"]:\n",
    "        part = (count % nr_parts)\n",
    "        count += 1\n",
    "        parts_data[part].append({\"repo_url\": repository[\"url\"], \"pull_request\": pull_request})\n",
    "\n",
    "for i in range(nr_parts):\n",
    "    with open(FILE_TO_SEPERATE.split(\".\")[0] + \"-part-\" + str(i+1) + \".json\", \"w\") as outfile:\n",
    "        json.dump(parts_data[i], outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98739107-471c-4478-960c-5baae19446b0",
   "metadata": {},
   "source": [
    "### File combinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8823ce5d-8c0f-4d9a-9c59-f4f62c8f69aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dict = {}\n",
    "combinator_data = []\n",
    "index_count = 0\n",
    "for part in range(nr_parts):\n",
    "    part_file = open(FILE_TO_SEPERATE.split(\".\")[0] + \"-part-\" + str(part+1) + \".json\", \"r\")\n",
    "    part_data = json.load(part_file)\n",
    "\n",
    "    for nugget in part_data:\n",
    "        # find repo using url\n",
    "        index = url_dict.get(nugget[\"repo_url\"], None)\n",
    "\n",
    "        if (index == None):\n",
    "            repo = {\"url\": nugget[\"repo_url\"], \"pull_requests\": []}\n",
    "            combinator_data.append(repo)\n",
    "            \n",
    "            url_dict[nugget[\"repo_url\"]] = index_count\n",
    "            index_count += 1\n",
    "        else:\n",
    "            repo = combinator_data[index]\n",
    "            \n",
    "        repo[\"pull_requests\"].append(nugget[\"pull_request\"])\n",
    "\n",
    "with open(FILE_TO_SEPERATE, \"w\") as outfile:\n",
    "    json.dump(combinator_data, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96031760-e0b7-4b4e-99ab-b75d1ad3ee10",
   "metadata": {},
   "source": [
    "## STEP 1: Pull request scraping script\n",
    "\n",
    "For each repository, get all pull request data. This includes PR Title, description, (review) comments and commit hashes. Exclude any repositories that do not have pull requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "492cb012-8537-42cb-8c58-3d9d45b21aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/andreas-prinz/gcp-terraform-google-lb to /repositories/425986937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/lwilliams1990/deepfence-threatmapper-lab to /repositories/270369845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/kmarilleau/a-cloud-guru-gcp-cloud-engineer-terraform to /repositories/323167041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/ryanlg/ryhino-public to /repositories/421638252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/Lemax-Dev/infrastructure-repo to /repositories/230511040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/globeandmail/aws-dynamodb to /repositories/231636188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/matthewbentley/blog-terraform to /repositories/113337906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/monish-advani/terragoat-test to /repositories/304530642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/dgorbov/terraform-s3-backend-setup to /repositories/230207730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    }
   ],
   "source": [
    "# SETTINGS\n",
    "check_limit_every_x_calls = 5\n",
    "api_limit_buffer = 10\n",
    "api_calls_per_debug = 500\n",
    "\n",
    "# INITIALIZATION\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# INITIALIZATION\n",
    "terraform_output = open(STEP4_TFCOMMITS)\n",
    "step4_output = json.load(terraform_output)\n",
    "\n",
    "# Retrieve data from previous run\n",
    "try:\n",
    "    previous_run = open(STEP5_TF_REPOS_WITH_PR)\n",
    "    repoData_dict = json.load(previous_run)\n",
    "except FileNotFoundError as e:\n",
    "    repoData_dict = []\n",
    "\n",
    "iteration = 0\n",
    "calls_till_next_debug = 0\n",
    "calls_till_limit_checkup = 0\n",
    "\n",
    "# Check for api limits, also periodically calls print debug.\n",
    "def CheckForApiLimit():\n",
    "    global calls_till_limit_checkup\n",
    "    global calls_till_next_debug\n",
    "    global api_calls_per_debug\n",
    "    global api_limit_buffer\n",
    "\n",
    "    # check for limit\n",
    "    if (calls_till_limit_checkup == 0):\n",
    "        core_limit = g.get_rate_limit().core\n",
    "\n",
    "        # sleep when exceeded api core limit\n",
    "        if (core_limit.remaining <= api_limit_buffer):\n",
    "            time_to_sleep = core_limit.raw_data['reset'] - time.time() + 1\n",
    "            print(\"Rate limit exceeded, sleeping for\", time_to_sleep, \"seconds.\", \"Actual remaining calls\", core_limit.remaining)\n",
    "            time.sleep(time_to_sleep)\n",
    "\n",
    "        calls_till_limit_checkup = check_limit_every_x_calls\n",
    "    \n",
    "    calls_till_limit_checkup -= 1\n",
    "\n",
    "    # check for debug\n",
    "    if (calls_till_next_debug == 0):\n",
    "        PrintDebug()\n",
    "        calls_till_next_debug = api_calls_per_debug\n",
    "\n",
    "    calls_till_next_debug -= 1\n",
    "\n",
    "# Prints debug message\n",
    "def PrintDebug():\n",
    "    global iteration\n",
    "    global repo_url\n",
    "\n",
    "    print(datetime.datetime.now().strftime(\"%H:%M:%S\"), \":\", \n",
    "              \"current iteration:\", iteration, \n",
    "              \"url:\", repo_url)\n",
    "\n",
    "# Pull request scraping script\n",
    "for rp in step4_output[\"repositories\"]:\n",
    "    try:\n",
    "        iteration += 1\n",
    "            \n",
    "        repo_url = rp[\"name\"]\n",
    "\n",
    "        # skip already scraped repositories\n",
    "        if any(d[\"url\"] == repo_url for d in repoData_dict):\n",
    "            continue\n",
    "\n",
    "        # Get the repo object from the url\n",
    "        split_list = repo_url.split(\"/\")\n",
    "        actual_url = (split_list[3]+ '/' + split_list[4]).split('.git')[0]\n",
    "        repo = g.get_repo(actual_url)\n",
    "        \n",
    "        # Get required info for pull requests\n",
    "        pull_requests_dict = []\n",
    "        pull_requests = repo.get_pulls(state=\"closed\")\n",
    "\n",
    "        if pull_requests.totalCount > 0:\n",
    "            for pr in pull_requests:\n",
    "    \n",
    "                # retrieve all review comments, not required if there are none.\n",
    "                comments = []\n",
    "            \n",
    "                for review in pr.get_reviews():\n",
    "                    if (review.body.strip() != \"\"):\n",
    "                        comments.append(review.body)\n",
    "                CheckForApiLimit()\n",
    "\n",
    "                if (pr.review_comments > 0):\n",
    "                    for review_comment in pr.get_review_comments():\n",
    "                        if (review_comment.body.strip() != \"\"):\n",
    "                            comments.append(review_comment.body)\n",
    "                    CheckForApiLimit()\n",
    "\n",
    "                if (pr.comments > 0):\n",
    "                    for comment in pr.get_issue_comments():\n",
    "                        if (comment.body.strip() != \"\"):\n",
    "                            comments.append(comment.body)\n",
    "                    CheckForApiLimit()\n",
    "    \n",
    "                # retrieve all connected commits.\n",
    "                commits = []\n",
    "                for commit in pr.get_commits():\n",
    "                    commits.append(commit.sha)\n",
    "                CheckForApiLimit()\n",
    "    \n",
    "                pull_requests_dict.append({\"url\": pr.html_url, \"title\": pr.title, \"body\": pr.body, \"comments\": comments, \"commits\": commits})\n",
    "            \n",
    "            CheckForApiLimit()        \n",
    "            repoData_dict.append({\"url\": repo_url, \"pull_requests\": pull_requests_dict});\n",
    "            \n",
    "            with open(STEP5_TF_REPOS_WITH_PR, \"w\") as outfile:\n",
    "                json.dump(repoData_dict, outfile)\n",
    "    except Exception as e:\n",
    "        print(\"exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c661aa7-2628-416b-9121-fc9d06060467",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## STEP 2: Find tf commits for tf repos with pr's \n",
    "\n",
    "For remaining repositories from step 5, collect all commits that modify a terraform file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa3cc0b8-8193-4bc1-821c-39080379904d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at iteration 0\n",
      "at iteration 50\n",
      "at iteration 100\n",
      "at iteration 150\n",
      "at iteration 200\n",
      "at iteration 250\n",
      "at iteration 300\n",
      "at iteration 350\n",
      "at iteration 400\n",
      "at iteration 450\n",
      "at iteration 500\n",
      "at iteration 550\n",
      "at iteration 600\n"
     ]
    }
   ],
   "source": [
    "from pydriller import Repository\n",
    "\n",
    "import json\n",
    "\n",
    "step5 = open(STEP5_TF_REPOS_WITH_PR)\n",
    "tf_repositories = json.load(step5)\n",
    "\n",
    "terraform_keywords = ['.tf', '.tf.json']\n",
    "\n",
    "iteration = 0\n",
    "    \n",
    "# Pull request scraping script\n",
    "repo_dic = []\n",
    "for repository in tf_repositories:\n",
    "    try:\n",
    "        if (iteration % 50 == 0):\n",
    "            print(\"at iteration\", iteration)\n",
    "            with open(STEP6_TF_REPOS_COMMITS, \"w\") as outfile:\n",
    "                json.dump(repo_dic, outfile)\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "        # Get each commit\n",
    "        commit_dic = []\n",
    "        for commit in Repository(repository[\"url\"]).traverse_commits():\n",
    "\n",
    "            modified_terraform = False\n",
    "            # find if it changes a terraform file\n",
    "            for file in commit.modified_files:\n",
    "                if any(key in file.filename for key in terraform_keywords):\n",
    "                    modified_terraform = True\n",
    "            \n",
    "            if modified_terraform:\n",
    "                commit_dic.append({\"hash\": commit.hash, \n",
    "                                   \"url\": repository[\"url\"].split(\".git\")[0] + \"/commit/\" + commit.hash, \n",
    "                                   \"date\": str(commit.author_date), \n",
    "                                   \"body\": commit.msg})\n",
    "  \n",
    "        repo_dic.append({\"url\":repository[\"url\"], \"commits\":commit_dic})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"exception:\", e)\n",
    "\n",
    "with open(STEP6_TF_REPOS_COMMITS, \"w\") as outfile:\n",
    "        json.dump(repo_dic, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdef562-2f31-4504-8ac0-dc95c2c1d27d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## STEP 3: exclude unrelated tf commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40d17f90-039c-49ab-9869-86012e6c82be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "step6 = open(STEP6_TF_REPOS_COMMITS)\n",
    "repository_commits = json.load(step6)\n",
    "\n",
    "labels_file = open(COMMIT_LABELS)\n",
    "commit_labels = json.load(labels_file)\n",
    "\n",
    "for repository in repository_commits:\n",
    "    commits = []\n",
    "    for commit in repository[\"commits\"]:\n",
    "        label = commit_labels.get(commit[\"hash\"], None)\n",
    "        \n",
    "        if label is None or \"unrelated\" not in label:\n",
    "            commits.append(commit)\n",
    "    repository[\"commits\"] = commits\n",
    "\n",
    "with open(STEP6A_TF_REPOS_RELEVANT_COMMITS, \"w\") as outfile:\n",
    "        json.dump(repository_commits, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa74b07b-f462-4646-a413-5747db1f864f",
   "metadata": {},
   "source": [
    "## STEP 4: filter out pull requests without relevant tf commit\n",
    "\n",
    "Removes any pull request that does not include a commit from the previous step, for the remaining pull requests, it combines the two datasets into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93e6ca7c-17ff-4858-a7a0-8b4530b4f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "step5 = open(STEP5_TF_REPOS_WITH_PR)\n",
    "repository_input = json.load(step5)\n",
    "\n",
    "step6a = open(STEP6A_TF_REPOS_RELEVANT_COMMITS)\n",
    "commit_input = json.load(step6a)\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "output_dict = []\n",
    "\n",
    "# for each repository\n",
    "for repository in repository_input:\n",
    "    # find commits for repo from step 6a\n",
    "    commit_input_list = next(repo[\"commits\"] for repo in commit_input if repo[\"url\"] == repository[\"url\"])\n",
    "\n",
    "    pr_dict = []\n",
    "    # for each pull request\n",
    "    for pull_request in repository[\"pull_requests\"]:\n",
    "        commit_dict = []\n",
    "\n",
    "        # for each commit\n",
    "        for commit_hash in pull_request[\"commits\"]:\n",
    "            # Find the exact commit from step 7\n",
    "            commit_data = next((commit for commit in commit_input_list if commit[\"hash\"] == commit_hash), None)\n",
    "            if (commit_data is not None):\n",
    "                commit_dict.append(commit_data)\n",
    "\n",
    "        pull_request[\"total_commits\"] = len(pull_request[\"commits\"])\n",
    "        pull_request[\"commits\"] = commit_dict\n",
    "        \n",
    "        if (len(commit_dict) > 0):\n",
    "            pr_dict.append(pull_request)\n",
    "    \n",
    "    if (len(pr_dict) > 0):\n",
    "        output_dict.append({\"url\": repository[\"url\"], \"pull_requests\": pr_dict})\n",
    "\n",
    "with open(STEP7_TF_REPOS_WITH_TF_PR, \"w\") as outfile:\n",
    "    json.dump(output_dict, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ca568-c990-49f9-b5ba-80d5345aff95",
   "metadata": {},
   "source": [
    "## STEP 5: list all tf pull request with a keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "922f5e46-7106-4995-b526-19b3030b0f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_keywords = [\"cheap\", \"expens\", \"cost\", \"efficient\", \"bill\", \"pay\"]\n",
    "\n",
    "step7 = open(STEP7_TF_REPOS_WITH_TF_PR)\n",
    "repo_input = json.load(step7)\n",
    "\n",
    "pullrequest_dict_output = []\n",
    "for repository in repo_input:\n",
    "    for pr in repository[\"pull_requests\"]:\n",
    "        \n",
    "        title   = True if (pr[\"title\"]        is not None and any(key in pr[\"title\"].lower()    for key in cost_keywords)) else False\n",
    "        body    = True if (pr[\"body\"]         is not None and any(key in pr[\"body\"].lower()     for key in cost_keywords)) else False\n",
    "        comment = True if (any(comment        is not None and     key in comment.lower()        for key in cost_keywords for comment in pr[\"comments\"])) else False\n",
    "        commit  = True if (any(commit[\"body\"] is not None and     key in commit[\"body\"].lower() for key in cost_keywords for commit  in pr[\"commits\"]))  else False\n",
    "            \n",
    "        reason = ((\"title \" if title else \"\") + \n",
    "                  (\"body \" if body else \"\") + \n",
    "                  (\"comment \" if comment else \"\") + \n",
    "                  (\"commit \" if commit else \"\"))\n",
    "        \n",
    "        if (title or body or comment or commit):\n",
    "            pullrequest_dict_output.append({\"reason\": reason.strip(), \"pull_request\": pr})\n",
    "\n",
    "with open(STEP8_TF_KEYWORD_PR, \"w\") as outfile:\n",
    "    json.dump(pullrequest_dict_output, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f216e1-5413-40cb-94fc-61185aa45963",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b231bb8-ced9-404b-890f-643e35b3df9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total repositories: 1278\n",
      "Of those that exist and have pull request(s): 610\n",
      "Of those that have relevant TF commits: 469\n",
      "\n",
      "Total TF PR's with a keyword: 814\n",
      "\n",
      "PR with keyword in:\t Only in:\n",
      "Title:\t\t 111 \t 30\n",
      "Description:\t 363 \t 214\n",
      "Comment:\t 354 \t 322\n",
      "commit message*: 194 \t 63\n",
      "\n",
      "*commits labeled as unrelevant have already been removed from the dataset, while the same is not true for the other locations.\n",
      "\n",
      "Total amount of relevant commits in TF PR's: 130720\n",
      "Of those that modify a TF file: 3266\n",
      "Of those that have a keyword: 203\n",
      "\n",
      "Amount of PR's with more than 250 commits (limit): 34\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "cost_keywords = [\"cheap\", \"expens\", \"cost\", \"efficient\", \"bill\", \"pay\"]\n",
    "\n",
    "step4 = open(STEP4_TFCOMMITS)\n",
    "step4_data = json.load(step4)\n",
    "\n",
    "step5 = open(STEP5_TF_REPOS_WITH_PR)\n",
    "step5_data = json.load(step5)\n",
    "\n",
    "step7 = open(STEP7_TF_REPOS_WITH_TF_PR)\n",
    "repo_input = json.load(step7)\n",
    "\n",
    "step8 = open(STEP8_TF_KEYWORD_PR)\n",
    "pr_reason_input = json.load(step8)\n",
    "\n",
    "\n",
    "# GENERAL REPOSITORY DATA\n",
    "\n",
    "print(\"Total repositories:\" , step4_data[\"no_of_repos\"])\n",
    "print(\"Of those that exist and have pull request(s):\", len(step5_data))\n",
    "print(\"Of those that have relevant TF commits:\", len(repo_input))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# GENERAL PULL REQUEST DATA\n",
    "print(\"Total TF PR's with a keyword:\", len(pr_reason_input))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"PR with keyword in:\\t\", \"Only in:\")\n",
    "print(\"Title:\\t\\t\", len([pr for pr in pr_reason_input if \"title\" in pr[\"reason\"]]), \"\\t\", len([pr for pr in pr_reason_input if \"title\" == pr[\"reason\"]]))\n",
    "print(\"Description:\\t\", len([pr for pr in pr_reason_input if \"body\" in pr[\"reason\"]]), \"\\t\", len([pr for pr in pr_reason_input if \"body\" == pr[\"reason\"]]))\n",
    "print(\"Comment:\\t\", len([pr for pr in pr_reason_input if \"comment\" in pr[\"reason\"]]), \"\\t\", len([pr for pr in pr_reason_input if \"comment\" == pr[\"reason\"]]))\n",
    "print(\"commit message*:\", len([pr for pr in pr_reason_input if \"commit\" in pr[\"reason\"]]), \"\\t\", len([pr for pr in pr_reason_input if \"commit\" == pr[\"reason\"]]))  \n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"*commits labeled as unrelevant have already been removed from the dataset, while the same is not true for the other locations.\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# GENERAL COMMIT DATA\n",
    "\n",
    "print(\"Total amount of relevant commits in TF PR's:\", sum([pr[\"total_commits\"] for repo in repo_input for pr in repo[\"pull_requests\"]]))\n",
    "print(\"Of those that modify a TF file:\", len([commit for pr_reason in pr_reason_input for commit in pr_reason[\"pull_request\"][\"commits\"]]))\n",
    "print(\"Of those that have a keyword:\", len([commit for pr_reason in pr_reason_input for commit in pr_reason[\"pull_request\"][\"commits\"] if any(key in commit[\"body\"].lower() for key in cost_keywords)]))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "count = 0\n",
    "for repo in repo_input:\n",
    "    for pr in repo[\"pull_requests\"]:\n",
    "        if (pr[\"total_commits\"] >= 250):\n",
    "            count += 1\n",
    "print(\"Amount of PR's with more than 250 commits (limit):\", count)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03773481-1f10-4e4f-a34d-f925d44491bb",
   "metadata": {},
   "source": [
    "## STEP 6: Parse to dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9d6a372-29a0-4e87-be59-64e0dfc68b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "step8 = open(STEP8_TF_KEYWORD_PR)\n",
    "pull_request_reasons = json.load(step8)\n",
    "\n",
    "pr_output = []\n",
    "for pull_request_reason in pull_request_reasons:\n",
    "    pull_request = pull_request_reason[\"pull_request\"]\n",
    "    commits = []\n",
    "    for commit in pull_request[\"commits\"]:\n",
    "        commits.append(commit[\"hash\"])\n",
    "    pr_output.append(\n",
    "        {\n",
    "            \"type\": \"pull_request\", \n",
    "            \"url\": pull_request[\"url\"],\n",
    "            \"content\": {\n",
    "                \"title\": pull_request[\"title\"],\n",
    "                \"body\": pull_request[\"body\"],\n",
    "                \"comments\": pull_request[\"comments\"],\n",
    "                \"commits\": commits\n",
    "                },\n",
    "            \"codes\": []\n",
    "        })\n",
    "\n",
    "with open(STEP9_TF_PR_DATASET, \"w\") as outfile:\n",
    "    json.dump(pr_output, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af530fa-093b-45c4-b558-43cbc7e82643",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## STEP 7: Manually label dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350e774-91a2-42dd-9f1e-8689de0611cd",
   "metadata": {},
   "source": [
    "This cannot be done through code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d052f09-1d43-415b-af96-8d1b375bafa8",
   "metadata": {},
   "source": [
    "## STEP 8: Filter out unrelated pull requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454231c2-db0a-44b2-bd4b-d8910c08f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "   \n",
    "data_file = open(STEP9_TF_PR_DATASET)\n",
    "data = json.load(data_file)\n",
    "\n",
    "output = []\n",
    "for pr in data:\n",
    "    if pr[\"codes\"] != [\"unrelated\"]:\n",
    "        output.append(pr)\n",
    "\n",
    "with open(STEP11_RELEVANT_DATASET , \"w\") as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207912fb-87a5-4a8d-9ed8-a44e71c19dbf",
   "metadata": {},
   "source": [
    "## STEP 12: label results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b3043361-1346-47fb-823a-9e0e32cd58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# retrieve and simplify pr dataset\n",
    "all_pull_requests_file = open(STEP9_TF_PR_DATASET)\n",
    "all_pull_requests = json.load(all_pull_requests_file)\n",
    "\n",
    "for pull_request in all_pull_requests:\n",
    "    labels = []\n",
    "\n",
    "    for label in pull_request[\"codes\"]:\n",
    "        if \"feature\" in label:\n",
    "            labels.append(\"feature\")\n",
    "        elif \"policy\" in label:\n",
    "            labels.append(\"policy\")\n",
    "        elif \"area\" in label:\n",
    "            labels.append(\"area\")\n",
    "        else:\n",
    "            labels.append(label)\n",
    "\n",
    "    labels.sort()\n",
    "    pull_request[\"codes\"] = labels\n",
    "\n",
    "# retrieve and simplify pr dataset\n",
    "pull_requests_file = open(STEP11_RELEVANT_DATASET)\n",
    "pull_requests = json.load(pull_requests_file)\n",
    "\n",
    "for pull_request in pull_requests:\n",
    "    labels = []\n",
    "\n",
    "    for label in pull_request[\"codes\"]:\n",
    "        if \"feature\" in label:\n",
    "            labels.append(\"feature\")\n",
    "        elif \"policy\" in label:\n",
    "            labels.append(\"policy\")\n",
    "        elif \"area\" in label:\n",
    "            labels.append(\"area\")\n",
    "        else:\n",
    "            labels.append(label)\n",
    "\n",
    "    labels.sort()\n",
    "    pull_request[\"codes\"] = labels\n",
    "\n",
    "# retrieve and simplify old commit dataset\n",
    "old_commits_file = open(OLD_COMMITS)\n",
    "old_commits = json.load(old_commits_file)\n",
    "old_commits = [commit for commit in old_commits if commit[\"type\"] == \"commit\"]\n",
    "\n",
    "# retrieve and simplify and append new commit dataset\n",
    "all_commits_file = open(NEW_COMMITS)\n",
    "all_commits = json.load(all_commits_file)\n",
    "all_commits = [commit for commit in all_commits if commit[\"codes\"] != [\"unrelated\"]]\n",
    "all_commits = old_commits + all_commits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6755e73-9341-4bdd-a84f-7fbe3fdc0e52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Label histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "98184fcb-fbd4-45e7-9adc-817f290dd4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label                |  old (%)   all (%)   prs (%)  \n",
      "\n",
      "instance             |  154 (28)  170 (28)   79 (31)\n",
      "saving               |  366 (68)  404 (66)  125 (49)\n",
      "awareness            |  203 (37)  213 (35)   87 (34)\n",
      "storage              |   72 (13)   82 (13)   62 (24)\n",
      "alert                |   45 (8)    59 (9)    33 (13)\n",
      "feature              |   51 (9)    67 (11)   67 (26)\n",
      "increase             |   12 (2)    18 (2)    21 (8) \n",
      "cluster              |   13 (2)    17 (2)    27 (10)\n",
      "networking           |   52 (9)    55 (9)    20 (7) \n",
      "provider             |   25 (4)    25 (4)     7 (2) \n",
      "billing_mode         |   29 (5)    32 (5)     9 (3) \n",
      "policy               |   11 (2)    16 (2)     6 (2) \n",
      "area                 |   14 (2)    14 (2)     3 (1) \n",
      "domain               |    8 (1)    10 (1)     0 (0) \n",
      "network              |    0 (0)     1 (0)     0 (0) \n"
     ]
    }
   ],
   "source": [
    "label_hist = {}\n",
    "\n",
    "for nugget in pull_requests:\n",
    "    for label in nugget[\"codes\"]:\n",
    "        if label_hist.get(label, None) is None:\n",
    "            label_hist[label] = {\"prs\": 0, \"old_commits\": 0, \"all_commits\": 0}\n",
    "\n",
    "        label_hist[label][\"prs\"] = label_hist[label][\"prs\"] + 1\n",
    "\n",
    "for nugget in old_commits:\n",
    "    for label in nugget[\"codes\"]:\n",
    "        if label_hist.get(label, None) is None:\n",
    "            label_hist[label] = {\"prs\": 0, \"old_commits\": 0, \"all_commits\": 0}\n",
    "\n",
    "        label_hist[label][\"old_commits\"] = label_hist[label][\"old_commits\"] + 1\n",
    "\n",
    "for nugget in all_commits:\n",
    "    for label in nugget[\"codes\"]:\n",
    "        if label_hist.get(label, None) is None:\n",
    "            label_hist[label] = {\"prs\": 0, \"old_commits\": 0, \"all_commits\": 0}\n",
    "\n",
    "        label_hist[label][\"all_commits\"] = label_hist[label][\"all_commits\"] + 1\n",
    "\n",
    "\n",
    "header = [\"label\", \"old (%)\", \"all (%)\", \"prs (%)\"]\n",
    "print('{:20} |  {:9} {:9} {:9}'.format(*header)) \n",
    "print()\n",
    "for label in label_hist:\n",
    "    row = [label, \n",
    "           label_hist[label][\"old_commits\"], \"(\" + str(int(label_hist[label][\"old_commits\"] / len(old_commits) * 100)) + \")\",\n",
    "           label_hist[label][\"all_commits\"], \"(\" + str(int(label_hist[label][\"all_commits\"] / len(all_commits) * 100)) + \")\",\n",
    "           label_hist[label][\"prs\"], \"(\" + str(int(label_hist[label][\"prs\"] / len(pull_requests) * 100)) + \")\"\n",
    "          ]\n",
    "    print('{:20} | {:4} {:4} {:4} {:4} {:4} {:4}'.format(*row)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f98552-d60f-4567-b3cf-b88260f16ade",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### label set histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6bf5bb06-441b-47e5-98af-78487399a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instance, saving': 25, 'awareness, storage': 10, 'alert': 28, 'awareness, feature': 6, 'awareness, instance': 10, 'saving, storage': 19, 'awareness, increase': 3, 'awareness, increase, storage': 3, 'saving': 10, 'awareness, saving, storage': 1, 'cluster, instance, saving': 4, 'networking, provider, saving': 1, 'awareness, feature, instance': 6, 'billing_mode, saving': 1, 'awareness, feature, increase': 3, 'billing_mode, feature, provider, storage': 1, 'feature, instance, saving': 14, 'increase, networking': 1, 'provider, saving': 2, 'billing_mode, storage': 4, 'awareness': 13, 'feature, saving, storage': 6, 'awareness, feature, storage': 2, 'provider, saving, storage': 1, 'awareness, feature, increase, storage': 3, 'networking, saving': 9, 'policy, saving, storage': 4, 'feature, policy, saving': 1, 'provider': 1, 'feature, instance, policy': 1, 'instance, networking, saving': 1, 'feature, instance, saving, storage': 2, 'awareness, cluster': 9, 'alert, feature': 2, 'area, awareness': 2, 'area, awareness, storage': 1, 'alert, awareness, networking': 1, 'cluster, feature, instance, saving': 2, 'awareness, cluster, instance': 1, 'awareness, feature, increase, instance': 1, 'awareness, provider': 1, 'awareness, billing_mode': 2, 'cluster, saving': 8, 'awareness, instance, storage': 2, 'billing_mode, feature, saving': 1, 'alert, feature, instance': 1, 'instance, saving, storage': 2, 'cluster, feature, saving': 1, 'feature, increase': 1, 'increase, instance': 3, 'feature, saving': 4, 'awareness, networking': 3, 'feature, increase, networking': 1, 'feature, networking, saving': 2, 'cluster, feature, saving, storage': 1, 'awareness, cluster, feature, instance': 1, 'alert, saving': 1, 'awareness, increase, instance, saving': 2, 'awareness, instance, networking': 1}\n"
     ]
    }
   ],
   "source": [
    "label_hist = {}\n",
    "\n",
    "for pull_request in pull_requests:\n",
    "    label = \", \".join(pull_request[\"codes\"])\n",
    "    label = label.replace(\"feature, feature\", \"feature\")\n",
    "    if label_hist.get(label, None) is None:\n",
    "        label_hist[label] = 1\n",
    "    else:\n",
    "        label_hist[label] = label_hist[label] + 1\n",
    "        \n",
    "\n",
    "print(label_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e439da75-9f48-49fb-b69d-f5ba7d44ab0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1c1afee1-61e4-40b3-850a-1d1e17800064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             | instance      saving        awareness     storage       alert         feature       increase      cluster       networking    provider      billing_mode  policy        area         \n",
      "instance     |                         52            24             6             1            31             6             8             2             0             0             1             0 \n",
      "saving       |           52                           3            36             1            37             2            16            13             4             2             5             0 \n",
      "awareness    |           24             3                          22             1            22            15            11             5             1             2             0             3 \n",
      "storage      |            6            36            22                           0            16             6             1             0             2             5             4             1 \n",
      "alert        |            1             1             1             0                           3             0             0             1             0             0             0             0 \n",
      "feature      |           31            37            22            16             3                           9             6             3             1             2             3             0 \n",
      "increase     |            6             2            15             6             0             9                           0             2             0             0             0             0 \n",
      "cluster      |            8            16            11             1             0             6             0                           0             0             0             0             0 \n",
      "networking   |            2            13             5             0             1             3             2             0                           1             0             0             0 \n",
      "provider     |            0             4             1             2             0             1             0             0             1                           1             0             0 \n",
      "billing_mode |            0             2             2             5             0             2             0             0             0             1                           0             0 \n",
      "policy       |            1             5             0             4             0             3             0             0             0             0             0                           0 \n",
      "area         |            0             0             3             1             0             0             0             0             0             0             0             0               \n"
     ]
    }
   ],
   "source": [
    "label_knowledge = {}\n",
    "\n",
    "for pull_request in pull_requests:\n",
    "    labels = pull_request[\"codes\"]\n",
    "    \n",
    "    for label in labels:\n",
    "    \n",
    "        if label_knowledge.get(label, None) is None:\n",
    "            label_knowledge[label] = {}\n",
    "    \n",
    "        for neighbour_label in labels:\n",
    "            if neighbour_label != label:\n",
    "                if label_knowledge[label].get(neighbour_label, None) is None:\n",
    "                    label_knowledge[label][neighbour_label] = 1\n",
    "                else:\n",
    "                    label_knowledge[label][neighbour_label] = label_knowledge[label][neighbour_label] + 1\n",
    "\n",
    "\n",
    "format_str = \"{:12} |\"\n",
    "header = [\"\"]\n",
    "\n",
    "for label in label_knowledge:\n",
    "    format_str += \" {:12} \"\n",
    "    header.append(label)\n",
    "\n",
    "print(format_str.format(*header))\n",
    "\n",
    "for label in label_knowledge:\n",
    "    row = [label]\n",
    "    for neighbour in label_knowledge:\n",
    "        if neighbour == label:\n",
    "            row.append(\"\")\n",
    "        elif label_knowledge[label].get(neighbour, None) is not None:\n",
    "            row.append(label_knowledge[label][neighbour])\n",
    "        else:\n",
    "            row.append(0)\n",
    "    print(format_str.format(*row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a003396-c0c2-4e8b-b259-c45f096e29f5",
   "metadata": {},
   "source": [
    "### pull request - commit intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "51dc68cb-bfca-409c-a101-4995ac2284da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of identical labels for pr and commits: 25\n",
      "pr labels: 305 common labels: 157 commit labels: 349\n"
     ]
    }
   ],
   "source": [
    "# first we need to combine pull requests with their respective commit hashes\n",
    "\n",
    "commits_per_pr_file = open(STEP8_TF_KEYWORD_PR)\n",
    "commits_per_pr = json.load(commits_per_pr_file)\n",
    "\n",
    "prs = []\n",
    "for commit_pr in commits_per_pr:\n",
    "\n",
    "    # Get labeled commits \n",
    "    commits = []\n",
    "    links = [commit_link[\"url\"] for commit_link in commit_pr[\"pull_request\"][\"commits\"]]\n",
    "    for commit in all_commits:\n",
    "        if commit[\"url\"] in links:\n",
    "            commits.append(commit)\n",
    "\n",
    "    # Get labeled pr\n",
    "    pr = None\n",
    "    for pull in all_pull_requests:\n",
    "        if pull[\"url\"] == commit_pr[\"pull_request\"][\"url\"]:\n",
    "            pr = pull\n",
    "\n",
    "    # turn commit labels into single ordered list of unique labels\n",
    "    commit_codes = []\n",
    "    for codes in [commit[\"codes\"] for commit in commits]:\n",
    "        commit_codes += codes\n",
    "    commit_codes = list(set(commit_codes))\n",
    "    commit_codes.sort()\n",
    "\n",
    "    # turn pr labels into ordered list of labels\n",
    "    pr_codes = pr[\"codes\"]\n",
    "    pr_codes.sort()\n",
    "\n",
    "    if len(commit_codes) > 0:\n",
    "        prs.append(\n",
    "            {\n",
    "                \"url\": pr[\"url\"],\n",
    "                \"pr-labels\": pr_codes,\n",
    "                \"commit-labels\": commit_codes,\n",
    "                \"commits\": len(commits)\n",
    "            })\n",
    "\n",
    "print(\"amount of identical labels for pr and commits:\", len([pr for pr in prs if pr[\"pr-labels\"] == pr[\"commit-labels\"]]))\n",
    "total_common = 0\n",
    "total_pr_count = 0\n",
    "total_commit_count = 0\n",
    "\n",
    "for pr in prs:\n",
    "    count = 0\n",
    "    for label in pr[\"pr-labels\"]:\n",
    "        if label in pr[\"commit-labels\"]:\n",
    "            count += 1\n",
    "    pr[\"common-labels\"] = count\n",
    "    total_common += count\n",
    "    total_pr_count += len(pr[\"pr-labels\"])\n",
    "    total_commit_count += len(pr[\"commit-labels\"])\n",
    "\n",
    "print(\"pr labels:\", total_pr_count, \"common labels:\", total_common, \"commit labels:\", total_commit_count)\n",
    "\n",
    "with open(COMMIT_PR_INTERSECTION , \"w\") as outfile:\n",
    "    json.dump(prs, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ab033-89b9-4150-a46a-7e633b1a266a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
