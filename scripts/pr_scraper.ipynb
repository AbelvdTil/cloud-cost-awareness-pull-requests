{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e03a43-7920-4679-a955-ed3118ef1201",
   "metadata": {},
   "source": [
    "# Pull request scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a05420a-77b4-4d26-acdd-b6ca46d0f239",
   "metadata": {},
   "source": [
    "## GitHub credentials\n",
    "A private access token is necessary to make use of less restrictive API limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18030255-77ee-4ed0-b11b-b2824e704349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: AbelvdTil\n"
     ]
    }
   ],
   "source": [
    "from github import RateLimitExceededException, Github\n",
    "\n",
    "# Providing access token\n",
    "access_token = \"\"\n",
    "g = Github(login_or_token=access_token)\n",
    "-\n",
    "# Confirm your login is successful\n",
    "user = g.get_user()\n",
    "print(f\"Authenticated as: {user.login}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09550648-f771-420c-bfbc-d3cd7bceec95",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5be4139-d47b-42f1-b4a6-eee6d6de92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "STEP4_TFCOMMITS = os.path.join(\"data\", \"step4-tf-commits.json\") \n",
    "STEP5_TF_PULLREQUESTS = os.path.join(\"data\", \"step5-tf-pullrequests.json\")\n",
    "STEP6_TF_REPOS_WITH_PR = os.path.join(\"data\", \"step6-tf-repos-with-pr.json\")\n",
    "STEP7_TF_REPOS_COMMITS = os.path.join(\"data\", \"step7-tf-repos-commits.json\")\n",
    "STEP8_TF_REPOS_WITH_TF_PR = os.path.join(\"data\", \"step8-tf-repos-with-tf-pr.json\")\n",
    "STEP9_TF_KEYWORD_PR = os.path.join(\"data\", \"step9-tf-keyword-pr.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dc4f3-4c41-422a-af1a-817915c2fb00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## File seperator and adder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93d27ae3-61d8-444e-99e5-bfffe7e66185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "FILE_TO_SEPERATE = STEP6_TF_REPOS_WITH_PR\n",
    "\n",
    "nr_parts = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8c9fe-a182-4600-bd3c-6e3fee8607a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### File seperator\n",
    "\n",
    "Output of step 5 will likely be too large for github, make sure each file is under 100MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac6757a0-cacc-4792-87d7-3ade5ff857f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = open(FILE_TO_SEPERATE)\n",
    "repoData_dict = json.load(results)\n",
    "\n",
    "size_per_part = math.ceil(len(repoData_dict) / nr_parts) \n",
    "\n",
    "part_data = []\n",
    "\n",
    "i = 0\n",
    "current_part = 0\n",
    "for rp in repoData_dict:\n",
    "    i += 1\n",
    "    part = math.floor(i / size_per_part)\n",
    "    if (part == current_part):\n",
    "        part_data.append(rp)\n",
    "    else:\n",
    "        part_data.append(rp)\n",
    "        with open(FILE_TO_SEPERATE.split(\".\")[0] + \"-part-\" + str(current_part+1) + \".json\", \"w\") as outfile:\n",
    "            json.dump(part_data, outfile) \n",
    "        part_data = []\n",
    "        current_part = part\n",
    "    \n",
    "if part_data != []:\n",
    "    with open(FILE_TO_SEPERATE.split(\".\")[0] + \"-part-\" + str(current_part+1) + \".json\", \"w\") as outfile:\n",
    "            json.dump(part_data, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98739107-471c-4478-960c-5baae19446b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### File combinator\n",
    "\n",
    "Combine the seperate parts back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8823ce5d-8c0f-4d9a-9c59-f4f62c8f69aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "repoData_dict = []\n",
    "for current_part in range(nr_parts):\n",
    "    part_file = open(STEP5_TF_PULLREQUESTS.split(\".\")[0] + \"-part-\" + str(current_part+1) + \".json\", \"r\")\n",
    "    part_data = json.load(part_file)\n",
    "\n",
    "    for rp in part_data:\n",
    "        repoData_dict.append(rp)\n",
    "\n",
    "with open(STEP5_TF_PULLREQUESTS, \"w\") as outfile:\n",
    "            json.dump(repoData_dict, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96031760-e0b7-4b4e-99ab-b75d1ad3ee10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## STEP 5: Pull request scraping script\n",
    "\n",
    "Also includes settings, initialization and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "492cb012-8537-42cb-8c58-3d9d45b21aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:44:46 : current iteration: 1 url: https://github.com/tkhoa2711/terraform-digitalocean.git\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    }
   ],
   "source": [
    "# SETTINGS\n",
    "ms_time_between_api_calls = 100\n",
    "check_limit_every_x_calls = 100\n",
    "api_calls_per_debug = 1000\n",
    "\n",
    "# INITIALIZATION\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "# INITIALIZATION\n",
    "terraform_output = open(STEP4_TFCOMMITS)\n",
    "step4_output = json.load(terraform_output)\n",
    "\n",
    "# Retrieve data from previous run\n",
    "try:\n",
    "    previous_run = open(STEP5_TF_PULLREQUESTS)\n",
    "    repoData_dict = json.load(previous_run)\n",
    "except FileNotFoundError as e:\n",
    "    repoData_dict = []\n",
    "\n",
    "iteration = 0\n",
    "calls_till_next_debug = 0\n",
    "calls_till_limit_checkup = 0\n",
    "\n",
    "# Check for api limits, also periodically calls print debug.\n",
    "def CheckForApiLimit():\n",
    "    global calls_till_limit_checkup\n",
    "    global calls_till_next_debug\n",
    "    global api_calls_per_debug\n",
    "\n",
    "    # check for limit\n",
    "    if (calls_till_limit_checkup == 0):\n",
    "        core_limit = g.get_rate_limit().core\n",
    "\n",
    "        # sleep when exceeded api core limit\n",
    "        if (core_limit.remaining <= check_limit_every_x_calls):\n",
    "            time_to_sleep = core_limit.raw_data['reset'] - time.time() + 1\n",
    "            print(\"Rate limit exceeded, sleeping for\", time_to_sleep, \"seconds.\", \"Actual remaining calls\", core_limit.remaining)\n",
    "            time.sleep(time_to_sleep)\n",
    "\n",
    "        calls_till_limit_checkup = check_limit_every_x_calls\n",
    "    \n",
    "    calls_till_limit_checkup -= 1\n",
    "\n",
    "    # check for debug\n",
    "    if (calls_till_next_debug == 0):\n",
    "        PrintDebug()\n",
    "        calls_till_next_debug = api_calls_per_debug\n",
    "\n",
    "    calls_till_next_debug -= 1\n",
    "\n",
    "# Prints debug message\n",
    "def PrintDebug():\n",
    "    global iteration\n",
    "    global repo_url\n",
    "\n",
    "    print(datetime.datetime.now().strftime(\"%H:%M:%S\"), \":\", \n",
    "              \"current iteration:\", iteration, \n",
    "              \"url:\", repo_url)\n",
    "\n",
    "# Pull request scraping script\n",
    "for rp in step4_output[\"repositories\"]:\n",
    "    try:\n",
    "        iteration += 1\n",
    "\n",
    "        if (iteration > 10):\n",
    "            break\n",
    "            \n",
    "        repo_url = rp[\"name\"]\n",
    "\n",
    "        # skip already scraped repositories\n",
    "        if any(d[\"url\"] == repo_url for d in repoData_dict):\n",
    "            continue\n",
    "\n",
    "        # Get the repo object from the url\n",
    "        split_list = repo_url.split(\"/\")\n",
    "        actual_url = (split_list[3]+ '/' + split_list[4]).split('.git')[0]\n",
    "        repo = g.get_repo(actual_url)\n",
    "        \n",
    "        # Get required info for pull requests\n",
    "        pull_requests_dict = []\n",
    "        for pr in repo.get_pulls(state=\"closed\"):\n",
    "\n",
    "            # retrieve all review comments, not required if there are none.\n",
    "            comments = []\n",
    "            if (pr.review_comments > 0):\n",
    "                for review in pr.get_reviews():\n",
    "                    if (review.body.strip() != \"\"):\n",
    "                        comments.append(review.body)\n",
    "                CheckForApiLimit()\n",
    "\n",
    "            # retrieve all connected commits.\n",
    "            commits = []\n",
    "            for commit in pr.get_commits():\n",
    "                commits.append(commit.sha)\n",
    "            CheckForApiLimit()\n",
    "\n",
    "            pull_requests_dict.append({\"url\": pr.html_url, \"title\": pr.title, \"body\": pr.body, \"comments\": comments, \"commits\": commits})\n",
    "        \n",
    "        CheckForApiLimit()        \n",
    "        repoData_dict.append({\"url\": repo_url, \"pull_requests\": pull_requests_dict});\n",
    "        \n",
    "        with open(STEP5_TF_PULLREQUESTS, \"w\") as outfile:\n",
    "            json.dump(repoData_dict, outfile)\n",
    "    except Exception as e:\n",
    "        print(\"exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16621013-7ada-4264-9bd1-b1aa8b5c234c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## STEP 6: Reduce to repositories with pull requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcd98b77-0f24-4a23-b82a-9bdb7f2df158",
   "metadata": {},
   "outputs": [],
   "source": [
    "step5 = open(STEP5_TF_PULLREQUESTS)\n",
    "step5_dict = json.load(step5)\n",
    "\n",
    "output_dict = []\n",
    "for rp in step5_dict:\n",
    "    if len(rp[\"pull_requests\"]) != 0:\n",
    "        output_dict.append({\"url\": rp[\"url\"], \"pull_requests\": rp[\"pull_requests\"]})\n",
    "\n",
    "with open(STEP6_TF_REPOS_WITH_PR, \"w\") as outfile:\n",
    "            json.dump(output_dict, outfile) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c661aa7-2628-416b-9121-fc9d06060467",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## STEP 7: Find tf commits for tf repos with pr's \n",
    "\n",
    "This will only store commits that modify a terraform file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa3cc0b8-8193-4bc1-821c-39080379904d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at iteration 0\n",
      "at iteration 50\n",
      "at iteration 100\n",
      "at iteration 150\n",
      "at iteration 200\n",
      "at iteration 250\n",
      "at iteration 300\n",
      "at iteration 350\n",
      "at iteration 400\n",
      "at iteration 450\n",
      "at iteration 500\n",
      "at iteration 550\n",
      "at iteration 600\n"
     ]
    }
   ],
   "source": [
    "from pydriller import Repository\n",
    "\n",
    "import json\n",
    "\n",
    "step6 = open(STEP6_TF_REPOS_WITH_PR)\n",
    "step6_dict = json.load(step6)\n",
    "\n",
    "terraform_keywords = ['.tf', '.tf.json']\n",
    "\n",
    "iteration = 0\n",
    "    \n",
    "# Pull request scraping script\n",
    "repo_dic = []\n",
    "for rp in step6_dict:\n",
    "    try:\n",
    "        if (iteration % 50 == 0):\n",
    "            print(\"at iteration\", iteration)\n",
    "            with open(STEP7_TF_REPOS_COMMITS, \"w\") as outfile:\n",
    "                json.dump(repo_dic, outfile)\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "        repo = Repository(rp[\"url\"])\n",
    "\n",
    "        # Get each commit\n",
    "        commit_dic = []\n",
    "        for commit in repo.traverse_commits():\n",
    "\n",
    "            modified_terraform = False\n",
    "            # find if it changes a terraform file\n",
    "            for file in commit.modified_files:\n",
    "                if any(key in file.filename for key in terraform_keywords):\n",
    "                    modified_terraform = True\n",
    "            \n",
    "            if modified_terraform:\n",
    "                commit_dic.append({\"hash\": commit.hash, \n",
    "                                   \"url\": rp[\"url\"].split(\".git\")[0] + \"/commit/\" + commit.hash, \n",
    "                                   \"date\": str(commit.author_date), \n",
    "                                   \"body\": commit.msg})\n",
    "  \n",
    "        repo_dic.append({\"url\":rp[\"url\"], \"commits\":commit_dic})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"exception:\", e)\n",
    "\n",
    "with open(STEP7_TF_REPOS_COMMITS, \"w\") as outfile:\n",
    "        json.dump(repo_dic, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa74b07b-f462-4646-a413-5747db1f864f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## STEP 8: filter out pull requests without tf commit\n",
    "\n",
    "Removes any pull request that does not include a commit from the previous step, for the remaining pull requests, it combines the two datasets into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93e6ca7c-17ff-4858-a7a0-8b4530b4f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import Repository\n",
    "\n",
    "import json\n",
    "\n",
    "step6 = open(STEP6_TF_REPOS_WITH_PR)\n",
    "repository_input = json.load(step6)\n",
    "\n",
    "step7 = open(STEP7_TF_REPOS_COMMITS)\n",
    "commit_input = json.load(step7)\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "output_dict = []\n",
    "\n",
    "# for each repository\n",
    "for rp in repository_input:\n",
    "    # find commits for repo from step 7\n",
    "    commit_input_list = next(repo[\"commits\"] for repo in commit_input if repo[\"url\"] == rp[\"url\"])\n",
    "\n",
    "    pr_dict = []\n",
    "    # for each pull request\n",
    "    for pr in rp[\"pull_requests\"]:\n",
    "        commit_dict = []\n",
    "\n",
    "        # for each commit\n",
    "        for commit_hash in pr[\"commits\"]:\n",
    "            # Find the exact commit from step 7\n",
    "            commit_data = next((commit for commit in commit_input_list if commit[\"hash\"] == commit_hash), None)\n",
    "            if (commit_data is not None):\n",
    "                commit_dict.append(commit_data)\n",
    "\n",
    "        \n",
    "        pr[\"commits\"] = commit_dict\n",
    "\n",
    "        if (len(commit_dict) > 0):\n",
    "            pr_dict.append(pr)\n",
    "    \n",
    "    if (len(pr_dict) > 0):\n",
    "        output_dict.append({\"url\": rp[\"url\"], \"pull_requests\": pr_dict})comment\n",
    "\n",
    "with open(STEP8_TF_REPOS_WITH_TF_PR, \"w\") as outfile:\n",
    "    json.dump(output_dict, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ca568-c990-49f9-b5ba-80d5345aff95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## STEP 9: list all tf pull request with a keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "922f5e46-7106-4995-b526-19b3030b0f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_keywords = [\"cheap\", \"expens\", \"cost\", \"efficient\", \"bill\", \"pay\"]\n",
    "\n",
    "step8 = open(STEP8_TF_REPOS_WITH_TF_PR)\n",
    "repo_input = json.load(step8)\n",
    "\n",
    "pullrequest_dict_output = []\n",
    "for repository in repo_input:\n",
    "    for pr in repository[\"pull_requests\"]:\n",
    "        \n",
    "        title   = True if (pr[\"title\"]        is not None and any(key in pr[\"title\"].lower() for key in cost_keywords)) else False\n",
    "        body    = True if (pr[\"body\"]         is not None and any(key in pr[\"body\"].lower()  for key in cost_keywords)) else False\n",
    "        comment = True if (any(comment        is not None and key in comment.lower()         for key in cost_keywords for comment in pr[\"comments\"])) else False\n",
    "        commit  = True if (any(commit[\"body\"] is not None and key in commit[\"body\"].lower()  for key in cost_keywords for commit in pr[\"commits\"])) else False\n",
    "            \n",
    "        reason = ((\"title \" if title else \"\") + \n",
    "                  (\"body \" if body else \"\") + \n",
    "                  (\"comment \" if comment else \"\") + \n",
    "                  (\"commit \" if commit else \"\"))\n",
    "        \n",
    "        if (title or body or comment or commit):\n",
    "            pullrequest_dict_output.append({\"reason\": reason.strip(), \"pull_request\": pr})\n",
    "\n",
    "with open(STEP9_TF_KEYWORD_PR, \"w\") as outfile:\n",
    "    json.dump(pullrequest_dict_output, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f216e1-5413-40cb-94fc-61185aa45963",
   "metadata": {},
   "source": [
    "## STEP 10: results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4b231bb8-ced9-404b-890f-643e35b3df9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How often pull requests are in the dataset for each reason: \n",
      "\n",
      "Total PR's found: 888\n",
      "PR's due to keyword in title: 283\n",
      "PR's due to keyword in description: 457\n",
      "PR's due to keyword in review comment: 16\n",
      "PR's due to keyword in commit message: 655\n",
      "\n",
      "How many commits in the dataset not because it's message contains a keyword.\n",
      "\n",
      "Total amount of commits: 5000\n",
      "Commits without a keyword: 4138\n"
     ]
    }
   ],
   "source": [
    "step9 = open(STEP9_TF_KEYWORD_PR)\n",
    "pr_reason_input = json.load(step9)\n",
    "\n",
    "print(\"How often pull requests are in the dataset for each reason: \\n\")\n",
    "\n",
    "print(\"Total PR's found:\", len(pr_reason_input))\n",
    "print(\"PR's due to keyword in title:\", len([pr for pr in pr_reason_input if \"title\" in pr[\"reason\"]]))\n",
    "print(\"PR's due to keyword in description:\", len([pr for pr in pr_reason_input if \"body\" in pr[\"reason\"]]))\n",
    "print(\"PR's due to keyword in review comment:\", len([pr for pr in pr_reason_input if \"comment\" in pr[\"reason\"]]))\n",
    "print(\"PR's due to keyword in commit message:\", len([pr for pr in pr_reason_input if \"commit\" in pr[\"reason\"]]))\n",
    "\n",
    "print(\"\\nHow many commits in the dataset not because it's message contains a keyword.\\n\")\n",
    "\n",
    "print(\"Total amount of commits:\", len([commit for pr_reason in pr_reason_input for commit in pr_reason[\"pull_request\"][\"commits\"]]))\n",
    "print(\"Commits without a keyword:\", len([commit for pr_reason in pr_reason_input for commit in pr_reason[\"pull_request\"][\"commits\"] if not any(key in commit[\"body\"].lower() for key in cost_keywords)]))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aefdaa-7ede-486e-bf13-8c447690dd48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
