{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e03a43-7920-4679-a955-ed3118ef1201",
   "metadata": {},
   "source": [
    "# Pull request scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a05420a-77b4-4d26-acdd-b6ca46d0f239",
   "metadata": {},
   "source": [
    "## GitHub credentials\n",
    "A private access token is necessary to make use of less restrictive API limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18030255-77ee-4ed0-b11b-b2824e704349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: AbelvdTil\n"
     ]
    }
   ],
   "source": [
    "from github import RateLimitExceededException, Github\n",
    "\n",
    "# Providing access token\n",
    "access_token = \"\"\n",
    "g = Github(login_or_token=access_token)\n",
    "\n",
    "# Confirm your login is successful\n",
    "user = g.get_user()\n",
    "print(f\"Authenticated as: {user.login}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09550648-f771-420c-bfbc-d3cd7bceec95",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5be4139-d47b-42f1-b4a6-eee6d6de92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "STEP4_TFCOMMITS = os.path.join(\"data\", \"previous-study\", \"step4-tf-commits.json\") \n",
    "COMMIT_LABELS = os.path.join(\"data\", \"process-labeled-commits\", \"full-commit-labels.json\") \n",
    "\n",
    "STEP5_TF_REPOS_WITH_PR = os.path.join(\"data\", \"pullrequest-scraping\", \"step5-tf-repos-with-pr.json\")\n",
    "STEP6_TF_REPOS_COMMITS = os.path.join(\"data\", \"pullrequest-scraping\", \"step6-tf-repos-commits.json\")\n",
    "STEP6A_TF_REPOS_RELEVANT_COMMITS = os.path.join(\"data\", \"pullrequest-scraping\", \"step6a-tf-repos-relevant-commits.json\")\n",
    "STEP7_TF_REPOS_WITH_TF_PR = os.path.join(\"data\", \"pullrequest-scraping\", \"step7-tf-repos-with-tf-pr.json\")\n",
    "STEP8_TF_KEYWORD_PR = os.path.join(\"data\", \"pullrequest-scraping\", \"step8-tf-keyword-pr.json\")\n",
    "STEP9_TF_PR_DATASET = os.path.join(\"data\", \"pullrequest-scraping\", \"step9-tf-pr-dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dc4f3-4c41-422a-af1a-817915c2fb00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## File seperator and combinator\n",
    "\n",
    "Files over 100MB are not stored on GitHub, therefore we need to seperate large files into smaller ones.\n",
    "Any step5 and step7 files can be split into multiple smaller ones and be combined together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8300a-3f1f-4b55-9c2f-319d2213d816",
   "metadata": {},
   "source": [
    "The repositories are split on pull request level. The first pull request is stored in part 1, the 2nd in part 2 etc. This will make sure that it is fairly equally distributed.\n",
    "\n",
    "Each pull request is accomodated with the url of the repository. Therefore it is possible to reconstruct the original file again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d27ae3-61d8-444e-99e5-bfffe7e66185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "FILE_TO_SEPERATE = STEP5_TF_REPOS_WITH_PR\n",
    "\n",
    "nr_parts = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8c9fe-a182-4600-bd3c-6e3fee8607a0",
   "metadata": {},
   "source": [
    "### File seperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac6757a0-cacc-4792-87d7-3ade5ff857f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_data = []\n",
    "\n",
    "for i in range(nr_parts):\n",
    "    parts_data.append([])\n",
    "\n",
    "file = open(FILE_TO_SEPERATE)\n",
    "seperator_data = json.load(file)\n",
    "\n",
    "count = 0\n",
    "for repository in seperator_data:\n",
    "    for pull_request in repository[\"pull_requests\"]:\n",
    "        part = (count % nr_parts)\n",
    "        count += 1\n",
    "        parts_data[part].append({\"repo_url\": repository[\"url\"], \"pull_request\": pull_request})\n",
    "\n",
    "for i in range(nr_parts):\n",
    "    with open(FILE_TO_SEPERATE.split(\".\")[0] + \"-part-\" + str(i+1) + \".json\", \"w\") as outfile:\n",
    "        json.dump(parts_data[i], outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98739107-471c-4478-960c-5baae19446b0",
   "metadata": {},
   "source": [
    "### File combinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8823ce5d-8c0f-4d9a-9c59-f4f62c8f69aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dict = {}\n",
    "combinator_data = []\n",
    "index_count = 0\n",
    "for part in range(nr_parts):\n",
    "    part_file = open(FILE_TO_SEPERATE.split(\".\")[0] + \"-part-\" + str(part+1) + \".json\", \"r\")\n",
    "    part_data = json.load(part_file)\n",
    "\n",
    "    for nugget in part_data:\n",
    "        # find repo using url\n",
    "        index = url_dict.get(nugget[\"repo_url\"], None)\n",
    "\n",
    "        if (index == None):\n",
    "            repo = {\"url\": nugget[\"repo_url\"], \"pull_requests\": []}\n",
    "            combinator_data.append(repo)\n",
    "            \n",
    "            url_dict[nugget[\"repo_url\"]] = index_count\n",
    "            index_count += 1\n",
    "        else:\n",
    "            repo = combinator_data[index]\n",
    "            \n",
    "        repo[\"pull_requests\"].append(nugget[\"pull_request\"])\n",
    "\n",
    "with open(FILE_TO_SEPERATE, \"w\") as outfile:\n",
    "    json.dump(combinator_data, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96031760-e0b7-4b4e-99ab-b75d1ad3ee10",
   "metadata": {},
   "source": [
    "## STEP 5: Pull request scraping script\n",
    "\n",
    "For each repository, get all pull request data. This includes PR Title, description, (review) comments and commit hashes. Exclude any repositories that do not have pull requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c2238b0-5a3e-46b4-b660-acf29d789133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice...', \"I think we'll need to ignore the replica count on the deployment, as this is something the pod autoscaler will adjust. E.g:\\r\\n\\r\\n```\\r\\nlifecycle {\\r\\n  ignore_changes = [spec.0.replicas]\\r\\n}\\r\\n```\\r\\n\\r\\nUnfortunately that probably means we'd have to either always enable the autoscaler, or create a separate version/copy of the deployment which has the ignore_changes. Keen for feedback on this!\", \"I'm thinking I'll just always enable autoscaling and set the default min/max pods to 1/1 - this should make it the same as the existing behaviour.\", 'Not sure I understand, are we not just turning on autoscaling for ambassador? I think it would feel cleaner to have it enabled with min/max set to 1/5.\\r\\n\\r\\nYes will probably have to add the ignore replicas (like kinesis-autoscaler?)', \"`ignore_changes` can't be set dynamically in terraform, so it's not very easy to get the `autoscaling_enable` var to work without creating a whole separate deployment with the `ignore_changes` directive in it. I was originally hoping to be able to make this change in a way that was backwards compatible, but I think it'll be a lot easier if I just enable autoscaling by default.\\r\\n\\r\\nHave added another commit which does the above, and also sets the default min/max pods to 3 and 6 respectively (that way we'll be able to get one per availability zone).\", \"I've removed the replica_count parameter - this won't be useful anymore. Will also squash this all into a single commit before I merge.\"]\n"
     ]
    }
   ],
   "source": [
    "repo_url = \"https://github.com/sailthru/terraform-kubernetes-ambassador.git\"\n",
    "\n",
    "\n",
    "# Get the repo object from the url\n",
    "split_list = repo_url.split(\"/\")\n",
    "actual_url = (split_list[3]+ '/' + split_list[4]).split('.git')[0]\n",
    "repo = g.get_repo(actual_url)\n",
    "\n",
    "\n",
    "pr = repo.get_pull(7)\n",
    "\n",
    "# Get required info for pull requests\n",
    "pull_requests_dict = []\n",
    "\n",
    "\n",
    "# retrieve all review comments, not required if there are none.\n",
    "comments = []\n",
    "\n",
    "for review in pr.get_reviews():\n",
    "    if (review.body.strip() != \"\"):\n",
    "        comments.append(review.body)\n",
    "\n",
    "for review_comment in pr.get_review_comments():\n",
    "    if (review_comment.body.strip() != \"\"):\n",
    "        comments.append(review_comment.body)\n",
    "\n",
    "for comment in pr.get_issue_comments():\n",
    "    if (comment.body.strip() != \"\"):\n",
    "        comments.append(comment.body)\n",
    "\n",
    "print(comments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405cedc-5871-4de2-8ebc-5988dcf5b1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "10:52:50 : current iteration: 18 url: https://github.com/aws-observability/aws-otel-test-framework.git\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "11:31:54 : current iteration: 22 url: https://github.com/coreos/tectonic-installer.git\n",
      "Rate limit exceeded, sleeping for 290.91007900238037 seconds. Actual remaining calls 4\n",
      "12:45:23 : current iteration: 28 url: https://github.com/ministryofjustice/modernisation-platform.git\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/guilhermerenew/infra-cost to /repositories/345402754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "Rate limit exceeded, sleeping for 298.6321346759796 seconds. Actual remaining calls 2\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/kmalkin/tf-aws-pi-hole to /repositories/299628670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:01:51 : current iteration: 68 url: https://github.com/rotemavni/smart_terravni.git\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "Rate limit exceeded, sleeping for 303.7807776927948 seconds. Actual remaining calls 5\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "15:22:03 : current iteration: 123 url: https://github.com/gordonmurray/terraform_aws_rds_secrets_manager.git\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/samq-ghdemo/terragoat to /repositories/373902314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Request GET /repos/hryze/kakeibo-app-terraform/pulls/7 failed with 403: Forbidden\n",
      "Setting next backoff to 330.508751s\n",
      "Request GET /repos/hryze/kakeibo-app-terraform/pulls/7 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n",
      "Request GET /repos/hryze/kakeibo-app-terraform/pulls/7 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n",
      "Request GET /repos/hryze/kakeibo-app-terraform/pulls/7 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n",
      "Request GET /repos/hryze/kakeibo-app-terraform/pulls/7 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n",
      "Request GET /repos/hryze/kakeibo-app-terraform/pulls/7 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n",
      "Request GET /repos/hryze/kakeibo-app-terraform/pulls/7 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/andreas-prinz/gcp-terraform-google-lb to /repositories/425986937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:07:32 : current iteration: 218 url: https://github.com/ExpediaGroup/apiary-data-lake.git\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/portefaix/portefaix to /repositories/308418167\n",
      "Request GET /repos/portefaix/portefaix-kubernetes/pulls/3165 failed with 403: Forbidden\n",
      "Setting next backoff to 326.188115s\n",
      "Request GET /repos/portefaix/portefaix-kubernetes/pulls/3165 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n",
      "Request GET /repos/portefaix/portefaix-kubernetes/pulls/3165 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n",
      "Request GET /repos/portefaix/portefaix-kubernetes/pulls/3165 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n",
      "Request GET /repos/portefaix/portefaix-kubernetes/pulls/3165 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n",
      "Request GET /repos/portefaix/portefaix-kubernetes/pulls/3165 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n",
      "Request GET /repos/portefaix/portefaix-kubernetes/pulls/3165 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n",
      "Following Github server redirection from /repos/lwilliams1990/deepfence-threatmapper-lab to /repositories/270369845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/SamTowne/BasketballDrillBot to /repositories/369647866\n",
      "Following Github server redirection from /repos/amezousan/serverless-blog-in-aws to /repositories/245259111\n",
      "Request GET /repositories/131585686/pulls/25/comments?page=2 failed with 403: Forbidden\n",
      "Setting next backoff to 288.642724s\n",
      "Request GET /repositories/131585686/pulls/25/comments?page=2 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n",
      "Request GET /repositories/131585686/pulls/25/comments?page=2 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n",
      "Request GET /repositories/131585686/pulls/25/comments?page=2 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n",
      "Request GET /repositories/131585686/pulls/25/comments?page=2 failed with 403: Forbidden\n",
      "Setting next backoff to 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/circleci/enterprise-setup to /repositories/56541073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "18:11:32 : current iteration: 302 url: https://github.com/travis-ci/terraform-config.git\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Request GET /repos/Crown-Commercial-Service/digitalmarketplace-aws/pulls/655 failed with 403: Forbidden\n",
      "Setting next backoff to 85.239603s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:02:58 : current iteration: 354 url: https://github.com/hmcts/cnp-module-app-service-plan.git\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/web3-storage/ipfs-elastic-provider-infrastructure to /repositories/429158380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/kmarilleau/a-cloud-guru-gcp-cloud-engineer-terraform to /repositories/323167041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Request GET /repos/GaloyMoney/charts/pulls/3703 failed with 403: Forbidden\n",
      "Setting next backoff to 369.745692s\n"
     ]
    }
   ],
   "source": [
    "# SETTINGS\n",
    "check_limit_every_x_calls = 5\n",
    "api_limit_buffer = 10\n",
    "api_calls_per_debug = 500\n",
    "\n",
    "# INITIALIZATION\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "# INITIALIZATION\n",
    "terraform_output = open(STEP5_TF_REPOS_WITH_PR)\n",
    "step4_output = json.load(terraform_output)\n",
    "\n",
    "# Retrieve data from previous run\n",
    "try:\n",
    "    previous_run = open(os.path.join(\"data\", \"pullrequest-scraping\", \"missing-review-comments.json\"))\n",
    "    repoData_dict = json.load(previous_run)\n",
    "except FileNotFoundError as e:\n",
    "    repoData_dict = []\n",
    "\n",
    "iteration = 0\n",
    "calls_till_next_debug = 0\n",
    "calls_till_limit_checkup = 0\n",
    "\n",
    "# Check for api limits, also periodically calls print debug.\n",
    "def CheckForApiLimit():\n",
    "    global calls_till_limit_checkup\n",
    "    global calls_till_next_debug\n",
    "    global api_calls_per_debug\n",
    "    global api_limit_buffer\n",
    "\n",
    "    # check for limit\n",
    "    if (calls_till_limit_checkup == 0):\n",
    "        core_limit = g.get_rate_limit().core\n",
    "\n",
    "        # sleep when exceeded api core limit\n",
    "        if (core_limit.remaining <= api_limit_buffer):\n",
    "            time_to_sleep = core_limit.raw_data['reset'] - time.time() + 1\n",
    "            print(\"Rate limit exceeded, sleeping for\", time_to_sleep, \"seconds.\", \"Actual remaining calls\", core_limit.remaining)\n",
    "            time.sleep(time_to_sleep)\n",
    "\n",
    "        calls_till_limit_checkup = check_limit_every_x_calls\n",
    "    \n",
    "    calls_till_limit_checkup -= 1\n",
    "\n",
    "    # check for debug\n",
    "    if (calls_till_next_debug == 0):\n",
    "        PrintDebug()\n",
    "        calls_till_next_debug = api_calls_per_debug\n",
    "\n",
    "    calls_till_next_debug -= 1\n",
    "\n",
    "# Prints debug message\n",
    "def PrintDebug():\n",
    "    global iteration\n",
    "    global repo_url\n",
    "\n",
    "    print(datetime.datetime.now().strftime(\"%H:%M:%S\"), \":\", \n",
    "              \"current iteration:\", iteration, \n",
    "              \"url:\", repo_url)\n",
    "\n",
    "# Pull request scraping script\n",
    "for rp in step4_output[\"repositories\"]:\n",
    "    try:\n",
    "        iteration += 1\n",
    "            \n",
    "        repo_url = rp[\"name\"]\n",
    "\n",
    "        # skip already scraped repositories\n",
    "        if any(d[\"url\"] == repo_url for d in repoData_dict):\n",
    "            continue\n",
    "\n",
    "        # Get the repo object from the url\n",
    "        split_list = repo_url.split(\"/\")\n",
    "        actual_url = (split_list[3]+ '/' + split_list[4]).split('.git')[0]\n",
    "        repo = g.get_repo(actual_url)\n",
    "        \n",
    "        # Get required info for pull requests\n",
    "        pull_requests_dict = []\n",
    "        pull_requests = repo.get_pulls(state=\"closed\")\n",
    "\n",
    "        for pr in pull_requests:\n",
    "\n",
    "            # retrieve all review comments, not required if there are none.\n",
    "            comments = []\n",
    "\n",
    "            if (pr.review_comments > 0):\n",
    "                for review_comment in pr.get_review_comments():\n",
    "                    if (review_comment.body.strip() != \"\"):\n",
    "                        comments.append(review_comment.body)\n",
    "                CheckForApiLimit()\n",
    "\n",
    "            if (comments != []):\n",
    "                pull_requests_dict.append({\"url\": pr.html_url, \"comments\": comments})\n",
    "        \n",
    "        CheckForApiLimit()        \n",
    "        repoData_dict.append({\"url\": repo_url, \"pull_requests\": pull_requests_dict});\n",
    "\n",
    "        with open( os.path.join(\"data\", \"pullrequest-scraping\", \"missing-review-comments.json\") , \"w\") as outfile:\n",
    "            json.dump(repoData_dict, outfile)\n",
    "    except Exception as e:\n",
    "        print(\"exception:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "492cb012-8537-42cb-8c58-3d9d45b21aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest\"}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m pull_requests_dict \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     82\u001b[0m pull_requests \u001b[38;5;241m=\u001b[39m repo\u001b[38;5;241m.\u001b[39mget_pulls(state\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpull_requests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotalCount\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pr \u001b[38;5;129;01min\u001b[39;00m pull_requests:\n\u001b[1;32m     86\u001b[0m \n\u001b[1;32m     87\u001b[0m         \u001b[38;5;66;03m# retrieve all review comments, not required if there are none.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m         comments \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/github/PaginatedList.py:192\u001b[0m, in \u001b[0;36mPaginatedList.totalCount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# set per_page = 1 so the totalCount is just the number of pages\u001b[39;00m\n\u001b[1;32m    191\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mper_page\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m})\n\u001b[0;32m--> 192\u001b[0m headers, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__requester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequestJsonAndCheck\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__firstUrl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__headers\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m headers:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_count\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/github/Requester.py:548\u001b[0m, in \u001b[0;36mRequester.requestJsonAndCheck\u001b[0;34m(self, verb, url, parameters, headers, input)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequestJsonAndCheck\u001b[39m(\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    542\u001b[0m     verb: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28minput\u001b[39m: Optional[Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    547\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Any]:\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__check(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequestJson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__customConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/github/Requester.py:713\u001b[0m, in \u001b[0;36mRequester.requestJson\u001b[0;34m(self, verb, url, parameters, headers, input, cnx)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__requestEncode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/github/Requester.py:810\u001b[0m, in \u001b[0;36mRequester.__requestEncode\u001b[0;34m(self, cnx, verb, url, parameters, requestHeaders, input, encode)\u001b[0m\n\u001b[1;32m    806\u001b[0m     requestHeaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m], encoded_input \u001b[38;5;241m=\u001b[39m encode(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNEW_DEBUG_FRAME(requestHeaders)\n\u001b[0;32m--> 810\u001b[0m status, responseHeaders, output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__requestRaw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequestHeaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Consts\u001b[38;5;241m.\u001b[39mheaderRateRemaining \u001b[38;5;129;01min\u001b[39;00m responseHeaders \u001b[38;5;129;01mand\u001b[39;00m Consts\u001b[38;5;241m.\u001b[39mheaderRateLimit \u001b[38;5;129;01min\u001b[39;00m responseHeaders:\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate_limiting \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# ints expected but sometimes floats returned: https://github.com/PyGithub/PyGithub/pull/2697\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(responseHeaders[Consts\u001b[38;5;241m.\u001b[39mheaderRateRemaining])),\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(responseHeaders[Consts\u001b[38;5;241m.\u001b[39mheaderRateLimit])),\n\u001b[1;32m    817\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/github/Requester.py:844\u001b[0m, in \u001b[0;36mRequester.__requestRaw\u001b[0;34m(self, cnx, verb, url, requestHeaders, input)\u001b[0m\n\u001b[1;32m    842\u001b[0m     cnx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__createConnection()\n\u001b[1;32m    843\u001b[0m cnx\u001b[38;5;241m.\u001b[39mrequest(verb, url, \u001b[38;5;28minput\u001b[39m, requestHeaders)\n\u001b[0;32m--> 844\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m status \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    847\u001b[0m responseHeaders \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m.\u001b[39mlower(): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mgetheaders()}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/github/Requester.py:196\u001b[0m, in \u001b[0;36mHTTPSRequestsConnectionClass.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m verb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverb\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m    195\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 196\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mverb\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RequestsResponse(r)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1368\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1367\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1370\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:317\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:278\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 278\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1271\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SETTINGS\n",
    "check_limit_every_x_calls = 5\n",
    "api_limit_buffer = 10\n",
    "api_calls_per_debug = 500\n",
    "\n",
    "# INITIALIZATION\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# INITIALIZATION\n",
    "terraform_output = open(STEP4_TFCOMMITS)\n",
    "step4_output = json.load(terraform_output)\n",
    "\n",
    "# Retrieve data from previous run\n",
    "try:\n",
    "    previous_run = open(STEP5_TF_REPOS_WITH_PR)\n",
    "    repoData_dict = json.load(previous_run)\n",
    "except FileNotFoundError as e:\n",
    "    repoData_dict = []\n",
    "\n",
    "iteration = 0\n",
    "calls_till_next_debug = 0\n",
    "calls_till_limit_checkup = 0\n",
    "\n",
    "# Check for api limits, also periodically calls print debug.\n",
    "def CheckForApiLimit():\n",
    "    global calls_till_limit_checkup\n",
    "    global calls_till_next_debug\n",
    "    global api_calls_per_debug\n",
    "    global api_limit_buffer\n",
    "\n",
    "    # check for limit\n",
    "    if (calls_till_limit_checkup == 0):\n",
    "        core_limit = g.get_rate_limit().core\n",
    "\n",
    "        # sleep when exceeded api core limit\n",
    "        if (core_limit.remaining <= api_limit_buffer):\n",
    "            time_to_sleep = core_limit.raw_data['reset'] - time.time() + 1\n",
    "            print(\"Rate limit exceeded, sleeping for\", time_to_sleep, \"seconds.\", \"Actual remaining calls\", core_limit.remaining)\n",
    "            time.sleep(time_to_sleep)\n",
    "\n",
    "        calls_till_limit_checkup = check_limit_every_x_calls\n",
    "    \n",
    "    calls_till_limit_checkup -= 1\n",
    "\n",
    "    # check for debug\n",
    "    if (calls_till_next_debug == 0):\n",
    "        PrintDebug()\n",
    "        calls_till_next_debug = api_calls_per_debug\n",
    "\n",
    "    calls_till_next_debug -= 1\n",
    "\n",
    "# Prints debug message\n",
    "def PrintDebug():\n",
    "    global iteration\n",
    "    global repo_url\n",
    "\n",
    "    print(datetime.datetime.now().strftime(\"%H:%M:%S\"), \":\", \n",
    "              \"current iteration:\", iteration, \n",
    "              \"url:\", repo_url)\n",
    "\n",
    "# Pull request scraping script\n",
    "for rp in step4_output[\"repositories\"]:\n",
    "    try:\n",
    "        iteration += 1\n",
    "            \n",
    "        repo_url = rp[\"name\"]\n",
    "\n",
    "        # skip already scraped repositories\n",
    "        if any(d[\"url\"] == repo_url for d in repoData_dict):\n",
    "            continue\n",
    "\n",
    "        # Get the repo object from the url\n",
    "        split_list = repo_url.split(\"/\")\n",
    "        actual_url = (split_list[3]+ '/' + split_list[4]).split('.git')[0]\n",
    "        repo = g.get_repo(actual_url)\n",
    "        \n",
    "        # Get required info for pull requests\n",
    "        pull_requests_dict = []\n",
    "        pull_requests = repo.get_pulls(state=\"closed\")\n",
    "\n",
    "        if pull_requests.totalCount > 0:\n",
    "            for pr in pull_requests:\n",
    "    \n",
    "                # retrieve all review comments, not required if there are none.\n",
    "                comments = []\n",
    "            \n",
    "                for review in pr.get_reviews():\n",
    "                    if (review.body.strip() != \"\"):\n",
    "                        comments.append(review.body)\n",
    "                CheckForApiLimit()\n",
    "\n",
    "                if (pr.review_comments > 0):\n",
    "                    for review_comment in pr.get_review_comments():\n",
    "                        if (review_comment.body.strip() != \"\"):\n",
    "                            comments.append(review_comment.body)\n",
    "                    CheckForApiLimit()\n",
    "\n",
    "                if (pr.comments > 0):\n",
    "                    for comment in pr.get_issue_comments():\n",
    "                        if (comment.body.strip() != \"\"):\n",
    "                            comments.append(comment.body)\n",
    "                    CheckForApiLimit()\n",
    "    \n",
    "                # retrieve all connected commits.\n",
    "                commits = []\n",
    "                for commit in pr.get_commits():\n",
    "                    commits.append(commit.sha)\n",
    "                CheckForApiLimit()\n",
    "    \n",
    "                pull_requests_dict.append({\"url\": pr.html_url, \"title\": pr.title, \"body\": pr.body, \"comments\": comments, \"commits\": commits})\n",
    "            \n",
    "            CheckForApiLimit()        \n",
    "            repoData_dict.append({\"url\": repo_url, \"pull_requests\": pull_requests_dict});\n",
    "            \n",
    "            with open(STEP5_TF_REPOS_WITH_PR, \"w\") as outfile:\n",
    "                json.dump(repoData_dict, outfile)\n",
    "    except Exception as e:\n",
    "        print(\"exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c661aa7-2628-416b-9121-fc9d06060467",
   "metadata": {},
   "source": [
    "## STEP 6: Find tf commits for tf repos with pr's \n",
    "\n",
    "For remaining repositories from step 5, collect all commits that modify a terraform file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa3cc0b8-8193-4bc1-821c-39080379904d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at iteration 0\n",
      "at iteration 50\n",
      "at iteration 100\n",
      "at iteration 150\n",
      "at iteration 200\n",
      "at iteration 250\n",
      "at iteration 300\n",
      "at iteration 350\n",
      "at iteration 400\n",
      "at iteration 450\n",
      "at iteration 500\n",
      "at iteration 550\n",
      "at iteration 600\n"
     ]
    }
   ],
   "source": [
    "from pydriller import Repository\n",
    "\n",
    "import json\n",
    "\n",
    "step5 = open(STEP5_TF_REPOS_WITH_PR)\n",
    "tf_repositories = json.load(step5)\n",
    "\n",
    "terraform_keywords = ['.tf', '.tf.json']\n",
    "\n",
    "iteration = 0\n",
    "    \n",
    "# Pull request scraping script\n",
    "repo_dic = []\n",
    "for repository in tf_repositories:\n",
    "    try:\n",
    "        if (iteration % 50 == 0):\n",
    "            print(\"at iteration\", iteration)\n",
    "            with open(STEP6_TF_REPOS_COMMITS, \"w\") as outfile:\n",
    "                json.dump(repo_dic, outfile)\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "        # Get each commit\n",
    "        commit_dic = []\n",
    "        for commit in Repository(repository[\"url\"]).traverse_commits():\n",
    "\n",
    "            modified_terraform = False\n",
    "            # find if it changes a terraform file\n",
    "            for file in commit.modified_files:\n",
    "                if any(key in file.filename for key in terraform_keywords):\n",
    "                    modified_terraform = True\n",
    "            \n",
    "            if modified_terraform:\n",
    "                commit_dic.append({\"hash\": commit.hash, \n",
    "                                   \"url\": repository[\"url\"].split(\".git\")[0] + \"/commit/\" + commit.hash, \n",
    "                                   \"date\": str(commit.author_date), \n",
    "                                   \"body\": commit.msg})\n",
    "  \n",
    "        repo_dic.append({\"url\":repository[\"url\"], \"commits\":commit_dic})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"exception:\", e)\n",
    "\n",
    "with open(STEP6_TF_REPOS_COMMITS, \"w\") as outfile:\n",
    "        json.dump(repo_dic, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdef562-2f31-4504-8ac0-dc95c2c1d27d",
   "metadata": {},
   "source": [
    "## STEP 6b: exclude unrelated tf commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40d17f90-039c-49ab-9869-86012e6c82be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "step6 = open(STEP6_TF_REPOS_COMMITS)\n",
    "repository_commits = json.load(step6)\n",
    "\n",
    "labels_file = open(COMMIT_LABELS)\n",
    "commit_labels = json.load(labels_file)\n",
    "\n",
    "for repository in repository_commits:\n",
    "    commits = []\n",
    "    for commit in repository[\"commits\"]:\n",
    "        label = commit_labels.get(commit[\"hash\"], None)\n",
    "        \n",
    "        if label is None or \"unrelated\" not in label:\n",
    "            commits.append(commit)\n",
    "    repository[\"commits\"] = commits\n",
    "\n",
    "with open(STEP6A_TF_REPOS_RELEVANT_COMMITS, \"w\") as outfile:\n",
    "        json.dump(repository_commits, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa74b07b-f462-4646-a413-5747db1f864f",
   "metadata": {},
   "source": [
    "## STEP 7: filter out pull requests without relevant tf commit\n",
    "\n",
    "Removes any pull request that does not include a commit from the previous step, for the remaining pull requests, it combines the two datasets into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93e6ca7c-17ff-4858-a7a0-8b4530b4f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "step5 = open(STEP5_TF_REPOS_WITH_PR)\n",
    "repository_input = json.load(step5)\n",
    "\n",
    "step6a = open(STEP6A_TF_REPOS_RELEVANT_COMMITS)\n",
    "commit_input = json.load(step6a)\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "output_dict = []\n",
    "\n",
    "# for each repository\n",
    "for repository in repository_input:\n",
    "    # find commits for repo from step 6a\n",
    "    commit_input_list = next(repo[\"commits\"] for repo in commit_input if repo[\"url\"] == repository[\"url\"])\n",
    "\n",
    "    pr_dict = []\n",
    "    # for each pull request\n",
    "    for pull_request in repository[\"pull_requests\"]:\n",
    "        commit_dict = []\n",
    "\n",
    "        # for each commit\n",
    "        for commit_hash in pull_request[\"commits\"]:\n",
    "            # Find the exact commit from step 7\n",
    "            commit_data = next((commit for commit in commit_input_list if commit[\"hash\"] == commit_hash), None)\n",
    "            if (commit_data is not None):\n",
    "                commit_dict.append(commit_data)\n",
    "\n",
    "        pull_request[\"total_commits\"] = len(pull_request[\"commits\"])\n",
    "        pull_request[\"commits\"] = commit_dict\n",
    "        \n",
    "        if (len(commit_dict) > 0):\n",
    "            pr_dict.append(pull_request)\n",
    "    \n",
    "    if (len(pr_dict) > 0):\n",
    "        output_dict.append({\"url\": repository[\"url\"], \"pull_requests\": pr_dict})\n",
    "\n",
    "with open(STEP7_TF_REPOS_WITH_TF_PR, \"w\") as outfile:\n",
    "    json.dump(output_dict, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ca568-c990-49f9-b5ba-80d5345aff95",
   "metadata": {},
   "source": [
    "## STEP 8: list all tf pull request with a keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "922f5e46-7106-4995-b526-19b3030b0f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_keywords = [\"cheap\", \"expens\", \"cost\", \"efficient\", \"bill\", \"pay\"]\n",
    "\n",
    "step7 = open(STEP7_TF_REPOS_WITH_TF_PR)\n",
    "repo_input = json.load(step7)\n",
    "\n",
    "pullrequest_dict_output = []\n",
    "for repository in repo_input:\n",
    "    for pr in repository[\"pull_requests\"]:\n",
    "        \n",
    "        title   = True if (pr[\"title\"]        is not None and any(key in pr[\"title\"].lower()    for key in cost_keywords)) else False\n",
    "        body    = True if (pr[\"body\"]         is not None and any(key in pr[\"body\"].lower()     for key in cost_keywords)) else False\n",
    "        comment = True if (any(comment        is not None and     key in comment.lower()        for key in cost_keywords for comment in pr[\"comments\"])) else False\n",
    "        commit  = True if (any(commit[\"body\"] is not None and     key in commit[\"body\"].lower() for key in cost_keywords for commit  in pr[\"commits\"]))  else False\n",
    "            \n",
    "        reason = ((\"title \" if title else \"\") + \n",
    "                  (\"body \" if body else \"\") + \n",
    "                  (\"comment \" if comment else \"\") + \n",
    "                  (\"commit \" if commit else \"\"))\n",
    "        \n",
    "        if (title or body or comment or commit):\n",
    "            pullrequest_dict_output.append({\"reason\": reason.strip(), \"pull_request\": pr})\n",
    "\n",
    "with open(STEP8_TF_KEYWORD_PR, \"w\") as outfile:\n",
    "    json.dump(pullrequest_dict_output, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03773481-1f10-4e4f-a34d-f925d44491bb",
   "metadata": {},
   "source": [
    "## STEP 9: Parse to dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9d6a372-29a0-4e87-be59-64e0dfc68b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "step8 = open(STEP8_TF_KEYWORD_PR)\n",
    "pull_request_reasons = json.load(step8)\n",
    "\n",
    "pr_output = []\n",
    "for pull_request_reason in pull_request_reasons:\n",
    "    pull_request = pull_request_reason[\"pull_request\"]\n",
    "    commits = []\n",
    "    for commit in pull_request[\"commits\"]:\n",
    "        commits.append(commit[\"hash\"])\n",
    "    pr_output.append(\n",
    "        {\n",
    "            \"type\": \"pull_request\", \n",
    "            \"url\": pull_request[\"url\"],\n",
    "            \"content\": {\n",
    "                \"title\": pull_request[\"title\"],\n",
    "                \"body\": pull_request[\"body\"],\n",
    "                \"comments\": pull_request[\"comments\"],\n",
    "                \"commits\": commits\n",
    "                },\n",
    "            \"codes\": []\n",
    "        })\n",
    "\n",
    "with open(STEP9_TF_PR_DATASET, \"w\") as outfile:\n",
    "    json.dump(pr_output, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f216e1-5413-40cb-94fc-61185aa45963",
   "metadata": {},
   "source": [
    "## STEP 9: results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b231bb8-ced9-404b-890f-643e35b3df9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total repositories: 1278\n",
      "Of those that exist and have pull request(s): 610\n",
      "Of those that have relevant TF commits: 469\n",
      "\n",
      "Total TF PR's with a keyword: 814\n",
      "\n",
      "PR with keyword in:\t Only in:\n",
      "Title:\t\t 111 \t 30\n",
      "Description:\t 363 \t 214\n",
      "Comment:\t 354 \t 322\n",
      "commit message*: 194 \t 63\n",
      "\n",
      "*commits labeled as unrelevant have already been removed from the dataset, while the same is not true for the other locations.\n",
      "\n",
      "Total amount of relevant commits in TF PR's: 130720\n",
      "Of those that modify a TF file: 3266\n",
      "Of those that have a keyword: 203\n",
      "\n",
      "Amount of PR's with more than 250 commits (limit): 34\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "cost_keywords = [\"cheap\", \"expens\", \"cost\", \"efficient\", \"bill\", \"pay\"]\n",
    "\n",
    "step4 = open(STEP4_TFCOMMITS)\n",
    "step4_data = json.load(step4)\n",
    "\n",
    "step5 = open(STEP5_TF_REPOS_WITH_PR)\n",
    "step5_data = json.load(step5)\n",
    "\n",
    "step7 = open(STEP7_TF_REPOS_WITH_TF_PR)\n",
    "repo_input = json.load(step7)\n",
    "\n",
    "step8 = open(STEP8_TF_KEYWORD_PR)\n",
    "pr_reason_input = json.load(step8)\n",
    "\n",
    "\n",
    "# GENERAL REPOSITORY DATA\n",
    "\n",
    "print(\"Total repositories:\" , step4_data[\"no_of_repos\"])\n",
    "print(\"Of those that exist and have pull request(s):\", len(step5_data))\n",
    "print(\"Of those that have relevant TF commits:\", len(repo_input))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# GENERAL PULL REQUEST DATA\n",
    "print(\"Total TF PR's with a keyword:\", len(pr_reason_input))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"PR with keyword in:\\t\", \"Only in:\")\n",
    "print(\"Title:\\t\\t\", len([pr for pr in pr_reason_input if \"title\" in pr[\"reason\"]]), \"\\t\", len([pr for pr in pr_reason_input if \"title\" == pr[\"reason\"]]))\n",
    "print(\"Description:\\t\", len([pr for pr in pr_reason_input if \"body\" in pr[\"reason\"]]), \"\\t\", len([pr for pr in pr_reason_input if \"body\" == pr[\"reason\"]]))\n",
    "print(\"Comment:\\t\", len([pr for pr in pr_reason_input if \"comment\" in pr[\"reason\"]]), \"\\t\", len([pr for pr in pr_reason_input if \"comment\" == pr[\"reason\"]]))\n",
    "print(\"commit message*:\", len([pr for pr in pr_reason_input if \"commit\" in pr[\"reason\"]]), \"\\t\", len([pr for pr in pr_reason_input if \"commit\" == pr[\"reason\"]]))  \n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"*commits labeled as unrelevant have already been removed from the dataset, while the same is not true for the other locations.\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# GENERAL COMMIT DATA\n",
    "\n",
    "print(\"Total amount of relevant commits in TF PR's:\", sum([pr[\"total_commits\"] for repo in repo_input for pr in repo[\"pull_requests\"]]))\n",
    "print(\"Of those that modify a TF file:\", len([commit for pr_reason in pr_reason_input for commit in pr_reason[\"pull_request\"][\"commits\"]]))\n",
    "print(\"Of those that have a keyword:\", len([commit for pr_reason in pr_reason_input for commit in pr_reason[\"pull_request\"][\"commits\"] if any(key in commit[\"body\"].lower() for key in cost_keywords)]))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "count = 0\n",
    "for repo in repo_input:\n",
    "    for pr in repo[\"pull_requests\"]:\n",
    "        if (pr[\"total_commits\"] >= 250):\n",
    "            count += 1\n",
    "print(\"Amount of PR's with more than 250 commits (limit):\", count)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48d916-2ec6-4755-bb90-c8ba09b53f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
