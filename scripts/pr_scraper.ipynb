{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e03a43-7920-4679-a955-ed3118ef1201",
   "metadata": {},
   "source": [
    "# Pull request scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a05420a-77b4-4d26-acdd-b6ca46d0f239",
   "metadata": {},
   "source": [
    "## GitHub credentials\n",
    "A private access token is necessary to make use of less restrictive API limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18030255-77ee-4ed0-b11b-b2824e704349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated as: AbelvdTil\n"
     ]
    }
   ],
   "source": [
    "from github import RateLimitExceededException, Github\n",
    "\n",
    "# Providing access token\n",
    "access_token = \"\"\n",
    "g = Github(login_or_token=access_token)\n",
    "\n",
    "# Confirm your login is successful\n",
    "user = g.get_user()\n",
    "print(f\"Authenticated as: {user.login}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09550648-f771-420c-bfbc-d3cd7bceec95",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5be4139-d47b-42f1-b4a6-eee6d6de92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "STEP4_TFCOMMITS = os.path.join(\"data\", \"step4-tf-commits.json\") \n",
    "STEP5_TF_PULLREQUESTS = os.path.join(\"data\", \"step5-tf-pullrequests.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96031760-e0b7-4b4e-99ab-b75d1ad3ee10",
   "metadata": {},
   "source": [
    "## Scraping script\n",
    "\n",
    "Also includes settings, initialization and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "492cb012-8537-42cb-8c58-3d9d45b21aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:44:46 : current iteration: 1 url: https://github.com/tkhoa2711/terraform-digitalocean.git\n",
      "exception: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/repos#get-a-repository\"}\n"
     ]
    }
   ],
   "source": [
    "# SETTINGS\n",
    "ms_time_between_api_calls = 100\n",
    "check_limit_every_x_calls = 100\n",
    "api_calls_per_debug = 1000\n",
    "\n",
    "# INITIALIZATION\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import pytz\n",
    "from pydriller import Repository\n",
    "\n",
    "\n",
    "# INITIALIZATION\n",
    "terraform_output = open(STEP4_TFCOMMITS)\n",
    "step4_output = json.load(terraform_output)\n",
    "\n",
    "# Retrieve data from previous run\n",
    "try:\n",
    "    previous_run = open(STEP5_TF_PULLREQUESTS)\n",
    "    repoData_dict = json.load(previous_run)\n",
    "except FileNotFoundError as e:\n",
    "    repoData_dict = []\n",
    "\n",
    "iteration = 0\n",
    "calls_till_next_debug = 0\n",
    "calls_till_limit_checkup = 0\n",
    "\n",
    "# Check for api limits, also periodically calls print debug.\n",
    "def CheckForApiLimit():\n",
    "    global calls_till_limit_checkup\n",
    "    global calls_till_next_debug\n",
    "    global api_calls_per_debug\n",
    "\n",
    "    # check for limit\n",
    "    if (calls_till_limit_checkup == 0):\n",
    "        core_limit = g.get_rate_limit().core\n",
    "\n",
    "        # sleep when exceeded api core limit\n",
    "        if (core_limit.remaining <= check_limit_every_x_calls):\n",
    "            time_to_sleep = core_limit.raw_data['reset'] - time.time() + 1\n",
    "            print(\"Rate limit exceeded, sleeping for\", time_to_sleep, \"seconds.\", \"Actual remaining calls\", core_limit.remaining)\n",
    "            time.sleep(time_to_sleep)\n",
    "\n",
    "        calls_till_limit_checkup = check_limit_every_x_calls\n",
    "    \n",
    "    calls_till_limit_checkup -= 1\n",
    "\n",
    "    # check for debug\n",
    "    if (calls_till_next_debug == 0):\n",
    "        PrintDebug()\n",
    "        calls_till_next_debug = api_calls_per_debug\n",
    "\n",
    "    calls_till_next_debug -= 1\n",
    "\n",
    "# Prints debug message\n",
    "def PrintDebug():\n",
    "    global iteration\n",
    "    global repo_url\n",
    "\n",
    "    print(datetime.datetime.now().strftime(\"%H:%M:%S\"), \":\", \n",
    "              \"current iteration:\", iteration, \n",
    "              \"url:\", repo_url)\n",
    "\n",
    "# Pull request scraping script\n",
    "for rp in step4_output[\"repositories\"]:\n",
    "    try:\n",
    "        iteration += 1\n",
    "\n",
    "        if (iteration > 10):\n",
    "            break\n",
    "            \n",
    "        repo_url = rp[\"name\"]\n",
    "\n",
    "        # skip already scraped repositories\n",
    "        if any(d[\"url\"] == repo_url for d in repoData_dict):\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Get the repo object from the url\n",
    "        split_list = repo_url.split(\"/\")\n",
    "        actual_url = (split_list[3]+ '/' + split_list[4]).split('.git')[0]\n",
    "        repo = g.get_repo(actual_url)\n",
    "        \n",
    "        # Get required info for pull requests\n",
    "        pull_requests_dict = []\n",
    "        for pr in repo.get_pulls(state=\"closed\"):\n",
    "\n",
    "            # retrieve all review comments, not required if there are none.\n",
    "            comments = []\n",
    "            if (pr.review_comments > 0):\n",
    "                for review in pr.get_reviews():\n",
    "                    if (review.body.strip() != \"\"):\n",
    "                        comments.append(review.body)\n",
    "                CheckForApiLimit()\n",
    "\n",
    "            # retrieve all connected commits.\n",
    "            commits = []\n",
    "            for commit in pr.get_commits():\n",
    "                commits.append(commit.sha)\n",
    "            CheckForApiLimit()\n",
    "\n",
    "            pull_requests_dict.append({\"url\": pr.html_url, \"title\": pr.title, \"body\": pr.body, \"comments\": comments, \"commits\": commits})\n",
    "        \n",
    "        CheckForApiLimit()        \n",
    "        repoData_dict.append({\"url\": repo_url, \"pull_requests\": pull_requests_dict});\n",
    "        \n",
    "        with open(STEP5_TF_PULLREQUESTS, \"w\") as outfile:\n",
    "            json.dump(repoData_dict, outfile)\n",
    "    except Exception as e:\n",
    "        print(\"exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8c9fe-a182-4600-bd3c-6e3fee8607a0",
   "metadata": {},
   "source": [
    "## File seperator\n",
    "\n",
    "Output of step 5 will likely be too large for github, make sure each file is under 100MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93d27ae3-61d8-444e-99e5-bfffe7e66185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "nr_parts = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac6757a0-cacc-4792-87d7-3ade5ff857f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = open(STEP5_TF_PULLREQUESTS)\n",
    "repoData_dict = json.load(results)\n",
    "\n",
    "size_per_part = math.ceil(len(repoData_dict) / nr_parts) \n",
    "\n",
    "part_data = []\n",
    "\n",
    "i = 0\n",
    "current_part = 0\n",
    "for rp in repoData_dict:\n",
    "    i += 1\n",
    "    part = math.floor(i / size_per_part)\n",
    "    if (part == current_part):\n",
    "        part_data.append(rp)\n",
    "    else:\n",
    "        part_data.append(rp)\n",
    "        with open(STEP5_TF_PULLREQUESTS.split(\".\")[0] + \"-part-\" + str(current_part+1) + \".json\", \"w\") as outfile:\n",
    "            json.dump(part_data, outfile) \n",
    "        part_data = []\n",
    "        current_part = part\n",
    "    \n",
    "if part_data != []:\n",
    "    with open(STEP5_TF_PULLREQUESTS.split(\".\")[0] + \"-part-\" + str(current_part+1) + \".json\", \"w\") as outfile:\n",
    "            json.dump(part_data, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98739107-471c-4478-960c-5baae19446b0",
   "metadata": {},
   "source": [
    "## File combinator\n",
    "\n",
    "Combine the seperate parts back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8823ce5d-8c0f-4d9a-9c59-f4f62c8f69aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "repoData_dict = []\n",
    "for current_part in range(nr_parts):\n",
    "    part_file = open(STEP5_TF_PULLREQUESTS.split(\".\")[0] + \"-part-\" + str(current_part+1) + \".json\", \"r\")\n",
    "    part_data = json.load(part_file)\n",
    "\n",
    "    for rp in part_data:\n",
    "        repoData_dict.append(rp)\n",
    "\n",
    "with open(STEP5_TF_PULLREQUESTS, \"w\") as outfile:\n",
    "            json.dump(repoData_dict, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16621013-7ada-4264-9bd1-b1aa8b5c234c",
   "metadata": {},
   "source": [
    "## Reduce to repositories with pull requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcd98b77-0f24-4a23-b82a-9bdb7f2df158",
   "metadata": {},
   "outputs": [],
   "source": [
    "step5 = open(STEP5_TF_PULLREQUESTS)\n",
    "step5_dict = json.load(step5)\n",
    "\n",
    "output_dict = []\n",
    "for rp in step5_dict:\n",
    "    output_dict.append({\"url\": rp[\"url\"], \"pull_requests\": rp[\"pull_requests\"]})\n",
    "\n",
    "with open(STEP5_TF_PULLREQUESTS, \"w\") as outfile:\n",
    "            json.dump(output_dict, outfile) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c7de2-ed5f-4865-9fd3-b6cee2e8898a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
